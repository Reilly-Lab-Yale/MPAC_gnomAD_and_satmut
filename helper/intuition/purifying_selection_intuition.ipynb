{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f687f047-1bab-42a1-87a0-8e66f277ed73",
   "metadata": {},
   "source": [
    "The goal of this analysis is to get an intuition for how genetic variant rarity changes with (predicted) genic consequence. \n",
    "\n",
    "This notebook will compute count tables of variant rarity category by ensembl predicted consequence, plus phylop & roulette scores (sum and sum of squares). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd96df-47f4-4bb8-b31f-2bccea1e58e0",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6ff270-68c8-45a0-a44e-db3effc51208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:39:57.075864Z",
     "iopub.status.busy": "2024-08-08T16:39:57.075594Z",
     "iopub.status.idle": "2024-08-08T16:39:57.830698Z",
     "shell.execute_reply": "2024-08-08T16:39:57.830187Z",
     "shell.execute_reply.started": "2024-08-08T16:39:57.075847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76f3b6-4007-4773-818e-32467f59104d",
   "metadata": {},
   "source": [
    "## create the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26ca28c-e7c1-42ef-a044-9a98fe5f595b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:03.503548Z",
     "iopub.status.busy": "2024-08-08T16:40:03.503038Z",
     "iopub.status.idle": "2024-08-08T16:40:09.496121Z",
     "shell.execute_reply": "2024-08-08T16:40:09.495648Z",
     "shell.execute_reply.started": "2024-08-08T16:40:03.503528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/08 12:40:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"purifying_selection\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce041ab-9b0a-4e5d-9945-b84990507982",
   "metadata": {},
   "source": [
    "## Load variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0379abb3-91ce-4e53-abf4-21146650e82c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:17.635389Z",
     "iopub.status.busy": "2024-08-08T16:40:17.635101Z",
     "iopub.status.idle": "2024-08-08T16:40:29.514527Z",
     "shell.execute_reply": "2024-08-08T16:40:29.514087Z",
     "shell.execute_reply.started": "2024-08-08T16:40:17.635371Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 12:40:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "#real data\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_data/2.3.add_transposons/*.csv.gz/*.csv.gz\")\n",
    "\n",
    "#toy data\n",
    "\n",
    "#df=spark.read \\\n",
    "#    .option(\"delimiter\",\"\\t\") \\\n",
    "#    .option(\"header\",\"true\") \\\n",
    "#    .csv(\"toy_with_missing_vep.tsv\")\n",
    "#OR\n",
    "#    .csv(\"toy.tsv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268c859-3610-4c10-99a2-266dc6138247",
   "metadata": {},
   "source": [
    "### Note that since we're tapping off variants at the 2.3 add transposon step, we're missing:\n",
    "- 2.5 filter : so we don't filter out exonic variants (this is desireable)\n",
    "- 3.0 pleio_and_filter : so we haven't dropped MAF_OR_AC_IS_ZERO (which is performed below)\n",
    "- 3.5 add_tf : (no great loss)\n",
    "- 3.6 remove non-snp (which we do below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c8189-c5f0-4fc0-948d-98b8571e7cfe",
   "metadata": {},
   "source": [
    "## Filter out non-SNP variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf62ccc-8cb6-438d-98c3-082a3b589b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:37.767060Z",
     "iopub.status.busy": "2024-08-08T16:40:37.766607Z",
     "iopub.status.idle": "2024-08-08T16:40:37.806040Z",
     "shell.execute_reply": "2024-08-08T16:40:37.805616Z",
     "shell.execute_reply.started": "2024-08-08T16:40:37.767043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= df.filter(\n",
    "     df.REF.isin(\"A\", \"T\", \"C\", \"G\") & df.ALT.isin(\"A\", \"T\", \"C\", \"G\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc6c7c-7a97-48f7-9f00-b923dce7f35c",
   "metadata": {},
   "source": [
    "# Filter out `MAF_OR_AC_IS_ZERO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3ea560-6d55-4a67-b7c1-90e97dd9596a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:41.126814Z",
     "iopub.status.busy": "2024-08-08T16:40:41.126549Z",
     "iopub.status.idle": "2024-08-08T16:40:41.135943Z",
     "shell.execute_reply": "2024-08-08T16:40:41.135528Z",
     "shell.execute_reply.started": "2024-08-08T16:40:41.126798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.filter(F.col(\"category\")!=\"MAF_OR_AC_IS_ZERO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a44bd-efd2-42d0-aa0a-d029a260a5cc",
   "metadata": {},
   "source": [
    "## Count occurances of each consequence code in each vep string.\n",
    "\n",
    "First, get a list of consequences for each variant. This is a little involved, because of the many layers we have to trawl through:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac671e37-25c3-4e39-81ac-d79791c1294f",
   "metadata": {},
   "source": [
    "![schema](./info_field.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44da45d6-42c1-49b3-82aa-9c287dbb00a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:44.533727Z",
     "iopub.status.busy": "2024-08-08T16:40:44.533459Z",
     "iopub.status.idle": "2024-08-08T16:40:44.616545Z",
     "shell.execute_reply": "2024-08-08T16:40:44.616097Z",
     "shell.execute_reply.started": "2024-08-08T16:40:44.533710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#semicolon split\n",
    "df=df.withColumn(\"info_split\",F.split(df[\"INFO\"],\";\"))\n",
    "df=df.withColumn(\"vep_alone\",F.expr(\"filter(info_split, x -> x LIKE 'vep=%')[0]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda77845-8b9a-4994-9f0e-fdbe693812cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:40:46.137951Z",
     "iopub.status.busy": "2024-08-08T16:40:46.137513Z",
     "iopub.status.idle": "2024-08-08T16:40:46.175869Z",
     "shell.execute_reply": "2024-08-08T16:40:46.175456Z",
     "shell.execute_reply.started": "2024-08-08T16:40:46.137934Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comma split\n",
    "df=df.withColumn(\"vep_split\",F.split(df[\"vep_alone\"],\",\"))\n",
    "\n",
    "#pipe split & grab first element. \n",
    "df = df.withColumn(\n",
    "    \"extracted_codes\",\n",
    "    F.transform(F.col(\"vep_split\"), lambda x: F.split(x, \"\\\\|\")[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d380b13d-2460-4e7a-a0e4-5cd9bd813109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:41:00.763205Z",
     "iopub.status.busy": "2024-08-08T16:41:00.762933Z",
     "iopub.status.idle": "2024-08-08T16:41:00.813729Z",
     "shell.execute_reply": "2024-08-08T16:41:00.813315Z",
     "shell.execute_reply.started": "2024-08-08T16:41:00.763185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#break up anpersand-ligated conseqence codes\n",
    "df = df.withColumn(\n",
    "    \"consq_codes\",\n",
    "    F.expr(\n",
    "        \"flatten(transform(extracted_codes, x -> IF(x IS NOT NULL, split(x, '&'), array())))\"\n",
    "    )\n",
    ")\n",
    "#Note the IS NOT NULL : the presence of comma-delimited protein information can bork a subset of the entries, which we need to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5542e98-4839-4bdb-9167-1dca32e33e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:41:03.222945Z",
     "iopub.status.busy": "2024-08-08T16:41:03.222522Z",
     "iopub.status.idle": "2024-08-08T16:41:03.231146Z",
     "shell.execute_reply": "2024-08-08T16:41:03.230750Z",
     "shell.execute_reply.started": "2024-08-08T16:41:03.222928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CHROM: string, POS: string, REF: string, ALT: string, ID: string, QUAL: string, FILTER: string, INFO: string, K562__ref: string, HepG2__ref: string, SKNSH__ref: string, K562__alt: string, HepG2__alt: string, SKNSH__alt: string, K562__skew: string, HepG2__skew: string, SKNSH__skew: string, AC: string, AN: string, AF: string, cadd_phred: string, is_in_dELS: string, is_in_CA: string, is_in_pELS: string, is_in_CA-H3K4me3: string, is_in_CA-CTCF: string, is_in_PLS: string, is_in_TF: string, is_in_CA-TF: string, P_ANNO: string, mean_ref: string, mean_skew: string, MAF: string, category: string, roulette_PN: string, roulette_MR: string, roulette_MG: string, in_rep: string, info_split: array<string>, vep_alone: string, vep_split: array<string>, extracted_codes: array<string>, consq_codes: array<string>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b834ed-ec16-4d98-9540-6dded14890cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:33.191962Z",
     "iopub.status.busy": "2024-07-26T15:49:33.191672Z",
     "iopub.status.idle": "2024-07-26T15:49:33.194230Z",
     "shell.execute_reply": "2024-07-26T15:49:33.193843Z",
     "shell.execute_reply.started": "2024-07-26T15:49:33.191942Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "#with open(\"dump\", \"w\") as file:\n",
    "#    for i in df.where(F.col(\"consq_codes\").isNull()).take(10):\n",
    "#    #for i in df.take(1000):\n",
    "#        file.write(str(i[\"vep_alone\"])+str(i[\"extracted_codes\"])+\"\\t\"+str(i[\"consq_codes\"])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fafbbd-0b4b-4f92-8fad-eee0a1789704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:46:02.474636Z",
     "iopub.status.busy": "2024-08-08T16:46:02.474328Z",
     "iopub.status.idle": "2024-08-08T16:46:02.497345Z",
     "shell.execute_reply": "2024-08-08T16:46:02.496913Z",
     "shell.execute_reply.started": "2024-08-08T16:46:02.474617Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Some variants might have no predicted consequences. We will use `absent`\n",
    "df=df.withColumn(\"consq_codes\", F.when(F.col(\"consq_codes\").isNull(), F.array(F.lit(\"absent\"))).otherwise(F.col(\"consq_codes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733a4fe-8583-4c6c-a760-60e65e53315f",
   "metadata": {},
   "source": [
    "Next, compute the worst consequence code for each var.\n",
    "\n",
    "I've retrieved consequences from [here](https://useast.ensembl.org/info/genome/variation/prediction/predicted_data.html) on 2024-06-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc72c14b-306d-4c3c-b659-903a367c51e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:46:04.277298Z",
     "iopub.status.busy": "2024-08-08T16:46:04.276846Z",
     "iopub.status.idle": "2024-08-08T16:46:04.281035Z",
     "shell.execute_reply": "2024-08-08T16:46:04.280672Z",
     "shell.execute_reply.started": "2024-08-08T16:46:04.277280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This order is taken from the website linked above, which states that \n",
    "#the codes are shown in order of severity (though it admits this is subjective)\n",
    "#I've assigned numbers, where the smaller the more severe\n",
    "\n",
    "consq_code_lut = {\"transcript_ablation\":0, \n",
    "                  \"splice_acceptor_variant\":1, \n",
    "                  \"splice_donor_variant\":2, \n",
    "                  \"stop_gained\":3, \n",
    "                  \"frameshift_variant\":4, \n",
    "                  \"stop_lost\":5, \n",
    "                  \"start_lost\":6, \n",
    "                  \"transcript_amplification\":7,\n",
    "                  \"feature_elongation\":8,\n",
    "                  \"feature_truncation\":9,\n",
    "                  \"inframe_insertion\":10,\n",
    "                  \"inframe_deletion\":11,\n",
    "                  \"missense_variant\":12,\n",
    "                  \"protein_altering_variant\":13,\n",
    "                  \"splice_donor_5th_base_variant\":14,\n",
    "                  \"splice_region_variant\":15,\n",
    "                  \"splice_donor_region_variant\":16,\n",
    "                  \"splice_polypyrimidine_tract_variant\":17,\n",
    "                  \"incomplete_terminal_codon_variant\":18,\n",
    "                  \"start_retained_variant\":19,\n",
    "                  \"stop_retained_variant\":20,\n",
    "                  \"synonymous_variant\":21,\n",
    "                  \"coding_sequence_variant\":22,\n",
    "                  \"mature_miRNA_variant\":23,\n",
    "                  \"5_prime_UTR_variant\":24,\n",
    "                  \"3_prime_UTR_variant\":25,\n",
    "                  \"non_coding_transcript_exon_variant\":26,\n",
    "                  \"intron_variant\":27,\n",
    "                  \"NMD_transcript_variant\":28,\n",
    "                  \"non_coding_transcript_variant\":29,\n",
    "                  \"coding_transcript_variant\":30,\n",
    "                  \"upstream_gene_variant\":31,\n",
    "                  \"downstream_gene_variant\":32,\n",
    "                  \"TFBS_ablation\":33,\n",
    "                  \"TFBS_amplification\":34,\n",
    "                  \"TF_binding_site_variant\":35,\n",
    "                  \"regulatory_region_ablation\":36,\n",
    "                  \"regulatory_region_amplification\":37,\n",
    "                  \"regulatory_region_variant\":38,\n",
    "                  \"intergenic_variant\":39,\n",
    "                  \"sequence_variant\":40,\n",
    "                  \"absent\":41\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6fb533e-9c63-4bfc-9b61-a652c3373864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:55:30.811016Z",
     "iopub.status.busy": "2024-08-08T16:55:30.810544Z",
     "iopub.status.idle": "2024-08-08T16:55:31.421961Z",
     "shell.execute_reply": "2024-08-08T16:55:31.421518Z",
     "shell.execute_reply.started": "2024-08-08T16:55:30.810999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# break into separate columns for individual enumeration\n",
    "\n",
    "\n",
    "for code in list(consq_code_lut.keys()):\n",
    "    df=df.withColumn(code,F.when(F.array_contains(F.col(\"consq_codes\"),code),F.lit(True)).otherwise(F.lit(False)))\n",
    "\n",
    "#list(possible_consequence_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee245120-b2a5-4d3f-b96c-3c84c692bc12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:56:14.394681Z",
     "iopub.status.busy": "2024-08-08T16:56:14.394409Z",
     "iopub.status.idle": "2024-08-08T16:56:18.211675Z",
     "shell.execute_reply": "2024-08-08T16:56:18.211245Z",
     "shell.execute_reply.started": "2024-08-08T16:56:14.394663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 12:56:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>ID</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>K562__ref</th>\n",
       "      <th>HepG2__ref</th>\n",
       "      <th>SKNSH__ref</th>\n",
       "      <th>K562__alt</th>\n",
       "      <th>HepG2__alt</th>\n",
       "      <th>SKNSH__alt</th>\n",
       "      <th>K562__skew</th>\n",
       "      <th>HepG2__skew</th>\n",
       "      <th>SKNSH__skew</th>\n",
       "      <th>AC</th>\n",
       "      <th>AN</th>\n",
       "      <th>AF</th>\n",
       "      <th>cadd_phred</th>\n",
       "      <th>is_in_dELS</th>\n",
       "      <th>is_in_CA</th>\n",
       "      <th>is_in_pELS</th>\n",
       "      <th>is_in_CA-H3K4me3</th>\n",
       "      <th>is_in_CA-CTCF</th>\n",
       "      <th>is_in_PLS</th>\n",
       "      <th>is_in_TF</th>\n",
       "      <th>is_in_CA-TF</th>\n",
       "      <th>P_ANNO</th>\n",
       "      <th>mean_ref</th>\n",
       "      <th>mean_skew</th>\n",
       "      <th>MAF</th>\n",
       "      <th>category</th>\n",
       "      <th>roulette_PN</th>\n",
       "      <th>roulette_MR</th>\n",
       "      <th>roulette_MG</th>\n",
       "      <th>in_rep</th>\n",
       "      <th>info_split</th>\n",
       "      <th>vep_alone</th>\n",
       "      <th>vep_split</th>\n",
       "      <th>extracted_codes</th>\n",
       "      <th>consq_codes</th>\n",
       "      <th>transcript_ablation</th>\n",
       "      <th>splice_acceptor_variant</th>\n",
       "      <th>splice_donor_variant</th>\n",
       "      <th>stop_gained</th>\n",
       "      <th>frameshift_variant</th>\n",
       "      <th>stop_lost</th>\n",
       "      <th>start_lost</th>\n",
       "      <th>transcript_amplification</th>\n",
       "      <th>feature_elongation</th>\n",
       "      <th>feature_truncation</th>\n",
       "      <th>inframe_insertion</th>\n",
       "      <th>inframe_deletion</th>\n",
       "      <th>missense_variant</th>\n",
       "      <th>protein_altering_variant</th>\n",
       "      <th>splice_donor_5th_base_variant</th>\n",
       "      <th>splice_region_variant</th>\n",
       "      <th>splice_donor_region_variant</th>\n",
       "      <th>splice_polypyrimidine_tract_variant</th>\n",
       "      <th>incomplete_terminal_codon_variant</th>\n",
       "      <th>start_retained_variant</th>\n",
       "      <th>stop_retained_variant</th>\n",
       "      <th>synonymous_variant</th>\n",
       "      <th>coding_sequence_variant</th>\n",
       "      <th>mature_miRNA_variant</th>\n",
       "      <th>5_prime_UTR_variant</th>\n",
       "      <th>3_prime_UTR_variant</th>\n",
       "      <th>non_coding_transcript_exon_variant</th>\n",
       "      <th>intron_variant</th>\n",
       "      <th>NMD_transcript_variant</th>\n",
       "      <th>non_coding_transcript_variant</th>\n",
       "      <th>coding_transcript_variant</th>\n",
       "      <th>upstream_gene_variant</th>\n",
       "      <th>downstream_gene_variant</th>\n",
       "      <th>TFBS_ablation</th>\n",
       "      <th>TFBS_amplification</th>\n",
       "      <th>TF_binding_site_variant</th>\n",
       "      <th>regulatory_region_ablation</th>\n",
       "      <th>regulatory_region_amplification</th>\n",
       "      <th>regulatory_region_variant</th>\n",
       "      <th>intergenic_variant</th>\n",
       "      <th>sequence_variant</th>\n",
       "      <th>absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr2</td>\n",
       "      <td>54064</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs994438331</td>\n",
       "      <td>.</td>\n",
       "      <td>PASS</td>\n",
       "      <td>K562__ref=-0.060518835;HepG2__ref=-0.11972282;...</td>\n",
       "      <td>-0.060518835</td>\n",
       "      <td>-0.11972282</td>\n",
       "      <td>-0.31378546</td>\n",
       "      <td>-0.061227016</td>\n",
       "      <td>-0.13887358</td>\n",
       "      <td>-0.30973908</td>\n",
       "      <td>-7.081867E-4</td>\n",
       "      <td>-0.019150767</td>\n",
       "      <td>0.00404637</td>\n",
       "      <td>2</td>\n",
       "      <td>152162</td>\n",
       "      <td>1.31439E-5</td>\n",
       "      <td>1.57</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.16467571258544922</td>\n",
       "      <td>-0.0052708617101113004</td>\n",
       "      <td>1.31439E-5</td>\n",
       "      <td>ULTRARARE</td>\n",
       "      <td>AGGCT</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.154</td>\n",
       "      <td>false</td>\n",
       "      <td>[K562__ref=-0.060518835, HepG2__ref=-0.1197228...</td>\n",
       "      <td>vep=A|intergenic_variant|MODIFIER|||Intergenic...</td>\n",
       "      <td>[vep=A|intergenic_variant|MODIFIER|||Intergeni...</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr2</td>\n",
       "      <td>90037</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>rs77219298</td>\n",
       "      <td>.</td>\n",
       "      <td>PASS</td>\n",
       "      <td>K562__ref=0.6132817;HepG2__ref=0.60334325;SKNS...</td>\n",
       "      <td>0.6132817</td>\n",
       "      <td>0.60334325</td>\n",
       "      <td>1.037919</td>\n",
       "      <td>0.6745551</td>\n",
       "      <td>0.64999133</td>\n",
       "      <td>1.084241</td>\n",
       "      <td>0.061273456</td>\n",
       "      <td>0.04664812</td>\n",
       "      <td>0.046321955</td>\n",
       "      <td>6130</td>\n",
       "      <td>152140</td>\n",
       "      <td>0.0402918</td>\n",
       "      <td>8.437</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.7515146732330322</td>\n",
       "      <td>0.05141450961430868</td>\n",
       "      <td>0.0402918</td>\n",
       "      <td>COMMON</td>\n",
       "      <td>CTATA</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.211</td>\n",
       "      <td>false</td>\n",
       "      <td>[K562__ref=0.6132817, HepG2__ref=0.60334325, S...</td>\n",
       "      <td>vep=G|intergenic_variant|MODIFIER|||Intergenic...</td>\n",
       "      <td>[vep=G|intergenic_variant|MODIFIER|||Intergeni...</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr2</td>\n",
       "      <td>127009</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>rs1382759750</td>\n",
       "      <td>.</td>\n",
       "      <td>PASS</td>\n",
       "      <td>K562__ref=0.28705546;HepG2__ref=0.62864524;SKN...</td>\n",
       "      <td>0.28705546</td>\n",
       "      <td>0.62864524</td>\n",
       "      <td>0.61934525</td>\n",
       "      <td>0.26176712</td>\n",
       "      <td>0.5995152</td>\n",
       "      <td>0.5896472</td>\n",
       "      <td>-0.025288366</td>\n",
       "      <td>-0.02913005</td>\n",
       "      <td>-0.029697992</td>\n",
       "      <td>4</td>\n",
       "      <td>148076</td>\n",
       "      <td>2.70132E-5</td>\n",
       "      <td>3.898</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5116819540659586</td>\n",
       "      <td>-0.028038802246252697</td>\n",
       "      <td>2.70132E-5</td>\n",
       "      <td>ULTRARARE</td>\n",
       "      <td>GGGGA</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.147</td>\n",
       "      <td>true</td>\n",
       "      <td>[K562__ref=0.28705546, HepG2__ref=0.62864524, ...</td>\n",
       "      <td>vep=A|intergenic_variant|MODIFIER|||Intergenic...</td>\n",
       "      <td>[vep=A|intergenic_variant|MODIFIER|||Intergeni...</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>[intergenic_variant]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr2</td>\n",
       "      <td>216454</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>rs1268178481</td>\n",
       "      <td>.</td>\n",
       "      <td>PASS</td>\n",
       "      <td>K562__ref=-0.051730506;HepG2__ref=0.043524552;...</td>\n",
       "      <td>-0.051730506</td>\n",
       "      <td>0.043524552</td>\n",
       "      <td>0.14609466</td>\n",
       "      <td>-0.061243348</td>\n",
       "      <td>0.035201576</td>\n",
       "      <td>0.13232395</td>\n",
       "      <td>-0.009512845</td>\n",
       "      <td>-0.008322978</td>\n",
       "      <td>-0.013770727</td>\n",
       "      <td>1</td>\n",
       "      <td>152222</td>\n",
       "      <td>6.56935E-6</td>\n",
       "      <td>6.16</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.04596290489037832</td>\n",
       "      <td>-0.010535517086585363</td>\n",
       "      <td>6.56935E-6</td>\n",
       "      <td>SINGLETON</td>\n",
       "      <td>TATAA</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.211</td>\n",
       "      <td>true</td>\n",
       "      <td>[K562__ref=-0.051730506, HepG2__ref=0.04352455...</td>\n",
       "      <td>vep=C|downstream_gene_variant|MODIFIER|SH3YL1|...</td>\n",
       "      <td>[vep=C|downstream_gene_variant|MODIFIER|SH3YL1...</td>\n",
       "      <td>[downstream_gene_variant, downstream_gene_vari...</td>\n",
       "      <td>[downstream_gene_variant, downstream_gene_vari...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr2</td>\n",
       "      <td>663375</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>rs886295094</td>\n",
       "      <td>.</td>\n",
       "      <td>PASS</td>\n",
       "      <td>K562__ref=0.3333961;HepG2__ref=0.54296434;SKNS...</td>\n",
       "      <td>0.3333961</td>\n",
       "      <td>0.54296434</td>\n",
       "      <td>0.24101157</td>\n",
       "      <td>0.3358715</td>\n",
       "      <td>0.599989</td>\n",
       "      <td>0.27866155</td>\n",
       "      <td>0.002475379</td>\n",
       "      <td>0.057024658</td>\n",
       "      <td>0.037650008</td>\n",
       "      <td>1</td>\n",
       "      <td>152248</td>\n",
       "      <td>6.56823E-6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>0.3724573453267415</td>\n",
       "      <td>0.03238334755102793</td>\n",
       "      <td>6.56823E-6</td>\n",
       "      <td>SINGLETON</td>\n",
       "      <td>GCATA</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.2</td>\n",
       "      <td>false</td>\n",
       "      <td>[K562__ref=0.3333961, HepG2__ref=0.54296434, S...</td>\n",
       "      <td>vep=G|downstream_gene_variant|MODIFIER|TMEM18|...</td>\n",
       "      <td>[vep=G|downstream_gene_variant|MODIFIER|TMEM18...</td>\n",
       "      <td>[downstream_gene_variant, downstream_gene_vari...</td>\n",
       "      <td>[downstream_gene_variant, downstream_gene_vari...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     POS REF ALT            ID QUAL FILTER  \\\n",
       "0  chr2   54064   G   A   rs994438331    .   PASS   \n",
       "1  chr2   90037   A   G    rs77219298    .   PASS   \n",
       "2  chr2  127009   G   A  rs1382759750    .   PASS   \n",
       "3  chr2  216454   T   C  rs1268178481    .   PASS   \n",
       "4  chr2  663375   A   G   rs886295094    .   PASS   \n",
       "\n",
       "                                                INFO     K562__ref  \\\n",
       "0  K562__ref=-0.060518835;HepG2__ref=-0.11972282;...  -0.060518835   \n",
       "1  K562__ref=0.6132817;HepG2__ref=0.60334325;SKNS...     0.6132817   \n",
       "2  K562__ref=0.28705546;HepG2__ref=0.62864524;SKN...    0.28705546   \n",
       "3  K562__ref=-0.051730506;HepG2__ref=0.043524552;...  -0.051730506   \n",
       "4  K562__ref=0.3333961;HepG2__ref=0.54296434;SKNS...     0.3333961   \n",
       "\n",
       "    HepG2__ref   SKNSH__ref     K562__alt   HepG2__alt   SKNSH__alt  \\\n",
       "0  -0.11972282  -0.31378546  -0.061227016  -0.13887358  -0.30973908   \n",
       "1   0.60334325     1.037919     0.6745551   0.64999133     1.084241   \n",
       "2   0.62864524   0.61934525    0.26176712    0.5995152    0.5896472   \n",
       "3  0.043524552   0.14609466  -0.061243348  0.035201576   0.13232395   \n",
       "4   0.54296434   0.24101157     0.3358715     0.599989   0.27866155   \n",
       "\n",
       "     K562__skew   HepG2__skew   SKNSH__skew    AC      AN          AF  \\\n",
       "0  -7.081867E-4  -0.019150767    0.00404637     2  152162  1.31439E-5   \n",
       "1   0.061273456    0.04664812   0.046321955  6130  152140   0.0402918   \n",
       "2  -0.025288366   -0.02913005  -0.029697992     4  148076  2.70132E-5   \n",
       "3  -0.009512845  -0.008322978  -0.013770727     1  152222  6.56935E-6   \n",
       "4   0.002475379   0.057024658   0.037650008     1  152248  6.56823E-6   \n",
       "\n",
       "  cadd_phred is_in_dELS is_in_CA is_in_pELS is_in_CA-H3K4me3 is_in_CA-CTCF  \\\n",
       "0       1.57      false    false      false            false         false   \n",
       "1      8.437      false    false      false            false         false   \n",
       "2      3.898      false     true      false            false         false   \n",
       "3       6.16      false    false      false            false         false   \n",
       "4      0.254       true    false      false            false         false   \n",
       "\n",
       "  is_in_PLS is_in_TF is_in_CA-TF  P_ANNO              mean_ref  \\\n",
       "0     false    false       false    -0.6  -0.16467571258544922   \n",
       "1     false    false       false   0.418    0.7515146732330322   \n",
       "2     false    false       false     0.0    0.5116819540659586   \n",
       "3     false     true       false   0.771   0.04596290489037832   \n",
       "4     false    false       false  -1.395    0.3724573453267415   \n",
       "\n",
       "                mean_skew         MAF   category roulette_PN roulette_MR  \\\n",
       "0  -0.0052708617101113004  1.31439E-5  ULTRARARE       AGGCT       0.151   \n",
       "1     0.05141450961430868   0.0402918     COMMON       CTATA       0.431   \n",
       "2   -0.028038802246252697  2.70132E-5  ULTRARARE       GGGGA       0.223   \n",
       "3   -0.010535517086585363  6.56935E-6  SINGLETON       TATAA       0.186   \n",
       "4     0.03238334755102793  6.56823E-6  SINGLETON       GCATA       0.198   \n",
       "\n",
       "  roulette_MG in_rep                                         info_split  \\\n",
       "0       0.154  false  [K562__ref=-0.060518835, HepG2__ref=-0.1197228...   \n",
       "1       0.211  false  [K562__ref=0.6132817, HepG2__ref=0.60334325, S...   \n",
       "2       0.147   true  [K562__ref=0.28705546, HepG2__ref=0.62864524, ...   \n",
       "3       0.211   true  [K562__ref=-0.051730506, HepG2__ref=0.04352455...   \n",
       "4         0.2  false  [K562__ref=0.3333961, HepG2__ref=0.54296434, S...   \n",
       "\n",
       "                                           vep_alone  \\\n",
       "0  vep=A|intergenic_variant|MODIFIER|||Intergenic...   \n",
       "1  vep=G|intergenic_variant|MODIFIER|||Intergenic...   \n",
       "2  vep=A|intergenic_variant|MODIFIER|||Intergenic...   \n",
       "3  vep=C|downstream_gene_variant|MODIFIER|SH3YL1|...   \n",
       "4  vep=G|downstream_gene_variant|MODIFIER|TMEM18|...   \n",
       "\n",
       "                                           vep_split  \\\n",
       "0  [vep=A|intergenic_variant|MODIFIER|||Intergeni...   \n",
       "1  [vep=G|intergenic_variant|MODIFIER|||Intergeni...   \n",
       "2  [vep=A|intergenic_variant|MODIFIER|||Intergeni...   \n",
       "3  [vep=C|downstream_gene_variant|MODIFIER|SH3YL1...   \n",
       "4  [vep=G|downstream_gene_variant|MODIFIER|TMEM18...   \n",
       "\n",
       "                                     extracted_codes  \\\n",
       "0                               [intergenic_variant]   \n",
       "1                               [intergenic_variant]   \n",
       "2                               [intergenic_variant]   \n",
       "3  [downstream_gene_variant, downstream_gene_vari...   \n",
       "4  [downstream_gene_variant, downstream_gene_vari...   \n",
       "\n",
       "                                         consq_codes  transcript_ablation  \\\n",
       "0                               [intergenic_variant]                False   \n",
       "1                               [intergenic_variant]                False   \n",
       "2                               [intergenic_variant]                False   \n",
       "3  [downstream_gene_variant, downstream_gene_vari...                False   \n",
       "4  [downstream_gene_variant, downstream_gene_vari...                False   \n",
       "\n",
       "   splice_acceptor_variant  splice_donor_variant  stop_gained  \\\n",
       "0                    False                 False        False   \n",
       "1                    False                 False        False   \n",
       "2                    False                 False        False   \n",
       "3                    False                 False        False   \n",
       "4                    False                 False        False   \n",
       "\n",
       "   frameshift_variant  stop_lost  start_lost  transcript_amplification  \\\n",
       "0               False      False       False                     False   \n",
       "1               False      False       False                     False   \n",
       "2               False      False       False                     False   \n",
       "3               False      False       False                     False   \n",
       "4               False      False       False                     False   \n",
       "\n",
       "   feature_elongation  feature_truncation  inframe_insertion  \\\n",
       "0               False               False              False   \n",
       "1               False               False              False   \n",
       "2               False               False              False   \n",
       "3               False               False              False   \n",
       "4               False               False              False   \n",
       "\n",
       "   inframe_deletion  missense_variant  protein_altering_variant  \\\n",
       "0             False             False                     False   \n",
       "1             False             False                     False   \n",
       "2             False             False                     False   \n",
       "3             False             False                     False   \n",
       "4             False             False                     False   \n",
       "\n",
       "   splice_donor_5th_base_variant  splice_region_variant  \\\n",
       "0                          False                  False   \n",
       "1                          False                  False   \n",
       "2                          False                  False   \n",
       "3                          False                  False   \n",
       "4                          False                  False   \n",
       "\n",
       "   splice_donor_region_variant  splice_polypyrimidine_tract_variant  \\\n",
       "0                        False                                False   \n",
       "1                        False                                False   \n",
       "2                        False                                False   \n",
       "3                        False                                False   \n",
       "4                        False                                False   \n",
       "\n",
       "   incomplete_terminal_codon_variant  start_retained_variant  \\\n",
       "0                              False                   False   \n",
       "1                              False                   False   \n",
       "2                              False                   False   \n",
       "3                              False                   False   \n",
       "4                              False                   False   \n",
       "\n",
       "   stop_retained_variant  synonymous_variant  coding_sequence_variant  \\\n",
       "0                  False               False                    False   \n",
       "1                  False               False                    False   \n",
       "2                  False               False                    False   \n",
       "3                  False               False                    False   \n",
       "4                  False               False                    False   \n",
       "\n",
       "   mature_miRNA_variant  5_prime_UTR_variant  3_prime_UTR_variant  \\\n",
       "0                 False                False                False   \n",
       "1                 False                False                False   \n",
       "2                 False                False                False   \n",
       "3                 False                False                False   \n",
       "4                 False                False                False   \n",
       "\n",
       "   non_coding_transcript_exon_variant  intron_variant  NMD_transcript_variant  \\\n",
       "0                               False           False                   False   \n",
       "1                               False           False                   False   \n",
       "2                               False           False                   False   \n",
       "3                               False           False                   False   \n",
       "4                               False           False                   False   \n",
       "\n",
       "   non_coding_transcript_variant  coding_transcript_variant  \\\n",
       "0                          False                      False   \n",
       "1                          False                      False   \n",
       "2                          False                      False   \n",
       "3                          False                      False   \n",
       "4                          False                      False   \n",
       "\n",
       "   upstream_gene_variant  downstream_gene_variant  TFBS_ablation  \\\n",
       "0                  False                    False          False   \n",
       "1                  False                    False          False   \n",
       "2                  False                    False          False   \n",
       "3                  False                     True          False   \n",
       "4                  False                     True          False   \n",
       "\n",
       "   TFBS_amplification  TF_binding_site_variant  regulatory_region_ablation  \\\n",
       "0               False                    False                       False   \n",
       "1               False                    False                       False   \n",
       "2               False                    False                       False   \n",
       "3               False                    False                       False   \n",
       "4               False                    False                       False   \n",
       "\n",
       "   regulatory_region_amplification  regulatory_region_variant  \\\n",
       "0                            False                      False   \n",
       "1                            False                      False   \n",
       "2                            False                      False   \n",
       "3                            False                      False   \n",
       "4                            False                       True   \n",
       "\n",
       "   intergenic_variant  sequence_variant  absent  \n",
       "0                True             False   False  \n",
       "1                True             False   False  \n",
       "2                True             False   False  \n",
       "3               False             False   False  \n",
       "4               False             False   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    display(df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2520b-559b-4654-a3d0-4a83a640f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute worst consequence code for bulk enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e68ff5-ce23-4974-893a-20bf205cb0ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:48.216772Z",
     "iopub.status.busy": "2024-07-26T15:49:48.216312Z",
     "iopub.status.idle": "2024-07-26T15:49:48.230436Z",
     "shell.execute_reply": "2024-07-26T15:49:48.230012Z",
     "shell.execute_reply.started": "2024-07-26T15:49:48.216755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lookup_broadcast = spark.sparkContext.broadcast(consq_code_lut)\n",
    "\n",
    "reverse_consequence_code_lut= {value: key for key, value in consq_code_lut.items()}\n",
    "\n",
    "#lookup_broadcast_reverse = spark.sparkContext.broadcast(reverse_consequence_code_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f37185-d7f1-40ee-aa3e-3b3a717a7e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:51.323494Z",
     "iopub.status.busy": "2024-07-26T15:49:51.323230Z",
     "iopub.status.idle": "2024-07-26T15:49:51.327944Z",
     "shell.execute_reply": "2024-07-26T15:49:51.327562Z",
     "shell.execute_reply.started": "2024-07-26T15:49:51.323477Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookup_transform(inp):\n",
    "    #turns a list of consequence codes into a list of severity ints\n",
    "    lookup=lookup_broadcast.value\n",
    "    return [lookup.get(item) for item in inp]\n",
    "\n",
    "def lookup_transform_reverse(inp):\n",
    "    #turns a SINGLE severity int into a consequence code\n",
    "    return reverse_consequence_code_lut.get(inp,\"ERR\")\n",
    "\n",
    "#register the UDFs\n",
    "lookup_transform_udf = F.udf(lookup_transform, returnType=T.ArrayType(T.IntegerType()))\n",
    "\n",
    "lookup_transform_reverse_udf = F.udf(lookup_transform_reverse, returnType=T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8d8afc-0186-4821-928b-155c89d13a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:52.107766Z",
     "iopub.status.busy": "2024-07-26T15:49:52.107341Z",
     "iopub.status.idle": "2024-07-26T15:49:52.137736Z",
     "shell.execute_reply": "2024-07-26T15:49:52.137274Z",
     "shell.execute_reply.started": "2024-07-26T15:49:52.107743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply the lookup UDF to convert string consequence codes to severity ints\n",
    "df=df.withColumn(\"consq_numeric\",lookup_transform_udf(df[\"consq_codes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfcdcb9-8a2c-49e8-837c-c574492c2e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:52.907627Z",
     "iopub.status.busy": "2024-07-26T15:49:52.907364Z",
     "iopub.status.idle": "2024-07-26T15:49:52.921607Z",
     "shell.execute_reply": "2024-07-26T15:49:52.921117Z",
     "shell.execute_reply.started": "2024-07-26T15:49:52.907611Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the worst severity score for each variant.\n",
    "df=df.withColumn(\"min_consq_numeric\", F.array_min(df[\"consq_numeric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c57fe5-f350-4e54-907c-a2318fdef51b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:53.810942Z",
     "iopub.status.busy": "2024-07-26T15:49:53.810662Z",
     "iopub.status.idle": "2024-07-26T15:49:53.829149Z",
     "shell.execute_reply": "2024-07-26T15:49:53.828707Z",
     "shell.execute_reply.started": "2024-07-26T15:49:53.810925Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert minimum consequence code back to string\n",
    "df=df.withColumn(\"worst_consq_string\",\n",
    "              lookup_transform_reverse_udf(df[\"min_consq_numeric\"])\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd45b80-962f-4ffb-b3ae-bb745d943f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:54.915903Z",
     "iopub.status.busy": "2024-07-26T15:49:54.915439Z",
     "iopub.status.idle": "2024-07-26T15:49:54.917887Z",
     "shell.execute_reply": "2024-07-26T15:49:54.917535Z",
     "shell.execute_reply.started": "2024-07-26T15:49:54.915887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#manual verification\n",
    "#import pandas as pd\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    display(df.limit(3).toPandas())\n",
    "#df.limit(3).toPandas()[\"consq_codes\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28972-b5e7-4f0f-9290-2de80cc5edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"phylop_significant\",F.col(\"P_ANNO\")>=2.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf8fcbc-bc0a-434b-8f42-8e9d2d9fde1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:49:55.939034Z",
     "iopub.status.busy": "2024-07-26T15:49:55.938774Z",
     "iopub.status.idle": "2024-07-26T15:49:55.987872Z",
     "shell.execute_reply": "2024-07-26T15:49:55.987430Z",
     "shell.execute_reply.started": "2024-07-26T15:49:55.939018Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count \n",
    "counts=df.groupBy(\"category\",\"worst_consq_string\",\"phylop_significant\",*list(consq_code_lut.keys())).agg(\n",
    "    \n",
    "    F.sum(\"P_ANNO\").alias(\"sum_phylop\"),\n",
    "    F.sum(F.col(\"P_ANNO\") * F.col(\"P_ANNO\")).alias(\"sum_of_squared_phylop\"),\n",
    "    \n",
    "    F.sum(\"roulette_MR\").alias(\"sum_roulette_MR\"),\n",
    "    F.sum(F.col(\"roulette_MR\") * F.col(\"roulette_MR\")).alias(\"sum_of_squared_roulette_MR\"),\n",
    "    \n",
    "    \n",
    "    F.count(\"*\").alias(\"count\")  # Count of elements in each group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce6f2e-2da5-4728-a2e0-f6fe75ff4772",
   "metadata": {},
   "source": [
    "Dump to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "914c858f-ef7a-40ba-9686-85615bcb6c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:11:20.394549Z",
     "iopub.status.busy": "2024-07-26T14:11:20.394234Z",
     "iopub.status.idle": "2024-07-26T14:11:20.447623Z",
     "shell.execute_reply": "2024-07-26T14:11:20.447034Z",
     "shell.execute_reply.started": "2024-07-26T14:11:20.394530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#COMMENTED OUT FOR DEBUG\n",
    "counts.coalesce(1).write.csv(\"counts_broken.csv\", mode=\"overwrite\", header=True)\n",
    "\n",
    "#array_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, T.ArrayType)]\n",
    "\n",
    "## Cast ARRAY<...> columns to strings\n",
    "#for col_name in array_columns:\n",
    "#    df = df.withColumn(col_name, F.concat_ws(\"-\", F.col(col_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92973eb1-de14-4b7e-a3d0-c09acaa9f12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T15:50:42.600658Z",
     "iopub.status.busy": "2024-07-26T15:50:42.600369Z",
     "iopub.status.idle": "2024-07-26T15:52:15.565400Z",
     "shell.execute_reply": "2024-07-26T15:52:15.564349Z",
     "shell.execute_reply.started": "2024-07-26T15:50:42.600639Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 11:50:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:KeyboardInterrupt while sending command.              (0 + 10) / 100]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/mcn26/.conda/envs/mcn_varef/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m collected_data\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworst_consq_string\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabsent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#collected_data=df.limit(5).collect()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#with open(\"dump_normal.tsv\", \"w\") as file:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdump.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Write the header\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==========>                                            (19 + 10) / 100]\r"
     ]
    }
   ],
   "source": [
    "#collected_data=df.where(df.worst_consq_string == \"absent\").limit(5).collect()\n",
    "##collected_data=df.limit(5).collect()\n",
    "##with open(\"dump_normal.tsv\", \"w\") as file:\n",
    "#with open(\"dump.tsv\", \"w\") as file:\n",
    "#    # Write the header\n",
    "#    header = \"\\t\".join(df.columns)\n",
    "#    file.write(header + \"\\n\")\n",
    "#    \n",
    "#    # Write the data rows\n",
    "#    for row in collected_data:\n",
    "#        line = \"\\t\".join(map(str, row))\n",
    "#        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87762122-d953-47f0-afac-83e317cc4643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T13:44:30.220642Z",
     "iopub.status.busy": "2024-07-26T13:44:30.220371Z",
     "iopub.status.idle": "2024-07-26T13:44:37.842400Z",
     "shell.execute_reply": "2024-07-26T13:44:37.841769Z",
     "shell.execute_reply.started": "2024-07-26T13:44:30.220623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                              (1 + 10) / 2258]\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/07/26 09:44:36 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\t... 15 more\n",
      "24/07/26 09:44:36 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/26 09:44:36 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/26 09:44:36 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Traceback (most recent call last):\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/vast/palmer/home.mccleary/mcn26/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/07/26 09:44:37 ERROR Executor: Exception in task 9.0 in stage 12.0 (TID 17237)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/07/26 09:44:37 ERROR TaskSetManager: Task 9 in stage 12.0 failed 1 times; aborting job\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 14.0 in stage 12.0 (TID 17242) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 12.0 in stage 12.0 (TID 17240) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 8.0 in stage 12.0 (TID 17236) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 11.0 in stage 12.0 (TID 17239) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 13.0 in stage 12.0 (TID 17241) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\nTypeError: 'NoneType' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworst_consq_string\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabsent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdumpABSENT.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/mcn_varef/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\nTypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 17228) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:37 WARN TaskSetManager: Lost task 3.0 in stage 12.0 (TID 17231) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:38 WARN TaskSetManager: Lost task 10.0 in stage 12.0 (TID 17238) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:38 WARN TaskSetManager: Lost task 2.0 in stage 12.0 (TID 17230) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/07/26 09:44:38 WARN TaskSetManager: Lost task 6.0 in stage 12.0 (TID 17234) (r814u23n03.mccleary.ycrc.yale.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 12.0 failed 1 times, most recent failure: Lost task 9.0 in stage 12.0 (TID 17237) (r814u23n03.mccleary.ycrc.yale.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3768885/4213338293.py\", line 4, in lookup_transform\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "#df.where(df.worst_consq_string == \"absent\").limit(5).write.csv(\"dumpABSENT.csv\", mode=\"overwrite\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
