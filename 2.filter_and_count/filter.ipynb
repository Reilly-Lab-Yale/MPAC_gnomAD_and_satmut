{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5941730-7061-4b4f-bf36-4615d23f1d2e",
   "metadata": {},
   "source": [
    "This notebook will:\n",
    "\n",
    "- Filter out the gorp\n",
    "    - Remove low-quality variants that don't pass GNOMAD's own filters.\n",
    "    - Remove low-quality variants not queried in a large number of individuals\n",
    "    - Remove variants with a MAF of 0 (they don't really \"vary\") in the population if they dont exist. ,\n",
    "    - Remove those variants that don't have all of the relevant metrics.\n",
    "        - For efficiency's sake, this latter task will be done throughout, denoted with $\\dagger$\n",
    "\n",
    "- $\\dagger$ remove variants with no PhyloP scores\n",
    "\n",
    "- Munge \"INFO\" field strings into...\n",
    "    - extract malinouis predictions columns\n",
    "    - extract ensembl VEP scores\n",
    "- Summarize\n",
    "    - malinouis predictions : \"mean skew\"\n",
    "    - $\\dagger$ remove variants with no \"mean skew\" malinouis predictions\n",
    "    - Summarize Ensembl VEP : \n",
    "- Compute allele-frequency category\n",
    "    - Compute \"rare\", \"ultra-rare\", \"common\", \"singleton\", etc...\n",
    "- Dump table to disc\n",
    "    \n",
    "\n",
    "Import all the stuff we will need & set up the spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0363d27-c5f7-4925-aa1f-e911f503be57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T19:17:58.887167Z",
     "iopub.status.busy": "2023-12-04T19:17:58.886614Z",
     "iopub.status.idle": "2023-12-04T19:17:59.195035Z",
     "shell.execute_reply": "2023-12-04T19:17:59.194632Z",
     "shell.execute_reply.started": "2023-12-04T19:17:58.887146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Filter\")\\\n",
    "\n",
    "# Create a SparkContext with the specified configurations\n",
    "if 'spark' in locals() and spark!=None:\n",
    "    spark.stop()\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a SparkSession from the SparkContext\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fcdbc-c7e1-467f-ad0a-c299ce85cda4",
   "metadata": {},
   "source": [
    "Load in annotated gnomad variants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b058554-c05c-4338-9f9c-562cc1e6d394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T19:18:07.868904Z",
     "iopub.status.busy": "2023-12-04T19:18:07.868434Z",
     "iopub.status.idle": "2023-12-04T19:18:09.667043Z",
     "shell.execute_reply": "2023-12-04T19:18:09.666518Z",
     "shell.execute_reply.started": "2023-12-04T19:18:07.868887Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_scripts/1.annotate/annotated.csv/*.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHROM\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP_ANNO\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelimiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_scripts/1.annotate/annotated.csv/*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_scripts/1.annotate/annotated.csv/*.csv."
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"CHROM\", StringType(), True),\n",
    "    StructField(\"POS\", StringType(), True),\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"REF\", StringType(), True),\n",
    "    StructField(\"ALT\", StringType(), True),\n",
    "    StructField(\"QUAL\", StringType(), True),\n",
    "    StructField(\"FILTER\", StringType(), True),\n",
    "    StructField(\"INFO\", StringType(), True),\n",
    "    StructField(\"P_ANNO\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_scripts/1.annotate/annotated.csv/*.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888e930-1aa5-414f-aac6-bb2e0bcaadd8",
   "metadata": {},
   "source": [
    "Extract relevant columns from the INFO field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62508751-cb1c-49c9-9bf0-b4206fe7710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####The `INFO` field contains a lot of useful information, but it is all smashed together into a string. \n",
    "#Let's extract information from that string. \n",
    "\n",
    "keys_to_extract = [#NONE CAN BE SUBSTRINGS OF THE OTHERS\n",
    "    \"K562__ref\", \"HepG2__ref\", \"SKNSH__ref\", \"K562__alt\", \"HepG2__alt\", \"SKNSH__alt\",\n",
    "    \"K562__skew\", \"HepG2__skew\", \"SKNSH__skew\", \"AC\", \"AN\", \"AF\", \"cadd_phred\", \"vep\",# \"P_ANNO\" already in its own column\n",
    "]\n",
    "\n",
    "# Apply the regexp_extract function to the DataFrame to create new columns for each key.\n",
    "# The expression '([^;]*)' captures any sequence of characters that are not a semicolon,\n",
    "# which is assumed to be the delimiter for the key-value pairs in the 'INFO' column.\n",
    "\n",
    "for key in keys_to_extract:\n",
    "\n",
    "    #df = df.withColumn(key, regexp_extract(col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1))\n",
    "    #when we find something put it, whne we don't put None\n",
    "    df = df.withColumn(key, \n",
    "                       when(\n",
    "                           F.regexp_extract(F.col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1) != \"\",\n",
    "                           F.regexp_extract(F.col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1)).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d4172-5e3e-4784-99dc-541ee9ada813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    #make sure we have the necessary population stats\n",
    "    (F.col(\"AF\").isNotNull()) &\n",
    "    (F.col(\"AC\").isNotNull()) &\n",
    "    (F.col(\"AN\").isNotNull()) &\n",
    "\n",
    "    #check variant has been queried in a reasonably large number of people\n",
    "    #approx 1/3 of pop size queried in this release of gnomad\n",
    "    #a little less conservative than gnomad's own warning threshold\n",
    "    #which is triggered when a vartiant is queried in < 1/2 population\n",
    "    (F.col(\"AN\").cast(\"int\") > 25385) &\n",
    "    \n",
    "    #gnomad filters passed. See original gnomad vcf header for spec.\n",
    "    (F.col(\"FILTER\") == \"PASS\") & \n",
    "    \n",
    "    \n",
    "    #(col(\"CHROM\") == \"chr22\") &\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c3d77-de1a-49d5-b825-7c881daab3d4",
   "metadata": {},
   "source": [
    "Compute mean malinouis reference activity and mean malinouis skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8d72f-4401-4195-b00d-16926a1f1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference activity\n",
    "\n",
    "df = df.withColumn(\"K562__ref\", F.col(\"K562__ref\").cast(\"float\"))\n",
    "df = df.withColumn(\"HepG2__ref\", F.col(\"HepG2__ref\").cast(\"float\"))\n",
    "df = df.withColumn(\"SKNSH__ref\", F.col(\"SKNSH__ref\").cast(\"float\"))\n",
    "\n",
    "df=df.withColumn(\"mean_ref\", (abs(F.col(\"K562__ref\")) + abs(F.col(\"HepG2__ref\")) + abs(F.col(\"SKNSH__ref\"))) / 3)\n",
    "\n",
    "#skew\n",
    "df = df.withColumn(\"K562__skew\", F.col(\"K562__skew\").cast(\"float\"))\n",
    "df = df.withColumn(\"HepG2__skew\", F.col(\"HepG2__skew\").cast(\"float\"))\n",
    "df = df.withColumn(\"SKNSH__skew\", F.col(\"SKNSH__skew\").cast(\"float\"))\n",
    "\n",
    "df=df.withColumn(\"mean_skew\", (abs(F.col(\"K562__skew\")) + abs(F.col(\"HepG2__skew\")) + abs(F.col(\"SKNSH__skew\"))) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdde57-f315-45fc-a76f-1324f7ec81ed",
   "metadata": {},
   "source": [
    "Extract VEP information.\n",
    "\n",
    "A single variant may have multiple effects. Thus, the VEP column has multiple entries. These are separated by commas. Within each comma-deliniated entry are multiple fields deliniated by bars (|). The second field of each entry contains the data we are interested in : the \"calculated variant consequence\" : basically a prediction of what the variant is likely to do. I've retrieved consequences from [here](https://useast.ensembl.org/info/genome/variation/prediction/predicted_data.html) on 2023-12-24. \n",
    "If something is in the `protein_coding` list, it's probably important, but irrelevant to the present study, which focuses on noncoding elements. If a variant has any `protein_coding` predictions, I will discard it. Otherwise, I simply count the number of occurances of all codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f99842-3383-42b5-a732-6f38ede76e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes=['transcript_ablation','splice_acceptor_variant','splice_donor_variant','stop_gained','frameshift_variant','stop_lost','start_lost','transcript_amplification','feature_elongation','feature_truncation','inframe_insertion','inframe_deletion','missense_variant','splice_donor_5th_base_variant','splice_region_variant','splice_donor_region_variant','splice_polypyrimidine_tract_variant','incomplete_terminal_codon_variant','start_retained_variant','synonymous_variant','coding_sequence_variant','mature_miRNA_variant','5_prime_UTR_variant','3_prime_UTR_variant','non_coding_transcript_exon_variant','intron_variant','NMD_transcript_variant','non_coding_transcript_variant','coding_transcript_variant','upstream_gene_variant','downstream_gene_variant','TFBS_ablation','TFBS_amplification','TF_binding_site_variant','regulatory_region_ablation','regulatory_region_amplification','regulatory_region_variant','intergenic_variant','sequence_variant']\n",
    "protein_coding=['protein_altering_variant']\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390b4d5-bb57-468d-8f30-68f40b1abe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'INFO' column if it's no longer needed.\n",
    "df = df.drop(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18992037-6a28-42c6-88ec-47617bead9fa",
   "metadata": {},
   "source": [
    "Let's compute a summary metric of VEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab50c598-cc50-44e7-942e-899eef0693aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T19:04:09.735252Z",
     "iopub.status.busy": "2023-11-30T19:04:09.734868Z",
     "iopub.status.idle": "2023-11-30T19:04:09.737554Z",
     "shell.execute_reply": "2023-11-30T19:04:09.737180Z",
     "shell.execute_reply.started": "2023-11-30T19:04:09.735232Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##write me..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250feef2-d914-4e52-a4d0-22e900b1a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variants ensembl VEP predicts to modify protein coding genes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
