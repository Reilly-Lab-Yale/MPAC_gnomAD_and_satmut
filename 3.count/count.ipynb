{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5941730-7061-4b4f-bf36-4615d23f1d2e",
   "metadata": {},
   "source": [
    "We will generate a number of count tables, described in sections below. \n",
    "\n",
    "## import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0363d27-c5f7-4925-aa1f-e911f503be57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:04:37.220334Z",
     "iopub.status.busy": "2024-01-23T21:04:37.220063Z",
     "iopub.status.idle": "2024-01-23T21:04:39.472677Z",
     "shell.execute_reply": "2024-01-23T21:04:39.472232Z",
     "shell.execute_reply.started": "2024-01-23T21:04:37.220317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67873089-a7f3-4ccf-8403-8a2413d84479",
   "metadata": {
    "tags": []
   },
   "source": [
    "## create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f220458-459e-4603-834f-9ee2fb2598cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:04:39.473903Z",
     "iopub.status.busy": "2024-01-23T21:04:39.473473Z",
     "iopub.status.idle": "2024-01-23T21:04:45.078210Z",
     "shell.execute_reply": "2024-01-23T21:04:45.077737Z",
     "shell.execute_reply.started": "2024-01-23T21:04:39.473887Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/23 16:04:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Count\")\\\n",
    "\n",
    "# Create a SparkContext with the specified configurations\n",
    "if 'spark' in locals() and spark!=None:\n",
    "    spark.stop()\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a SparkSession from the SparkContext\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fcdbc-c7e1-467f-ad0a-c299ce85cda4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in gnomad variants filtered in the last script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0c646e-25e9-4d80-901f-f33e83cc7f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:04:45.079335Z",
     "iopub.status.busy": "2024-01-23T21:04:45.079115Z",
     "iopub.status.idle": "2024-01-23T21:05:12.573763Z",
     "shell.execute_reply": "2024-01-23T21:05:12.573250Z",
     "shell.execute_reply.started": "2024-01-23T21:04:45.079319Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/23 16:04:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#loading in all autosomes\n",
    "#Skipping sex chromosomes, see readme\n",
    "df = spark.read \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(\"/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_data/2.filter/*.csv/*.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821e580-d5eb-43ab-960a-5ded4f2fca68",
   "metadata": {},
   "source": [
    "## cast columns to the appropriate types & Drop columns rows with null values. \n",
    "\n",
    "Dropping isn't strictly necessary. We could, for example, only drop those rows with null malinouis skew when computing malinouis-skew-based metrics, drop rows with no phyloP scores when computing phyloP-based metrics, etc etc. However, this would result in different sets of variants summarized by each graph, which could create biases : if, for example, PhyloP scores are annotated for a nonrandom set of variants. Therefore I will drop rows with null data in any relevant columns prior to subsequent analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38dd9f2e-712c-4e74-9a01-3969e82de5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:12.574905Z",
     "iopub.status.busy": "2024-01-23T21:05:12.574672Z",
     "iopub.status.idle": "2024-01-23T21:05:12.612562Z",
     "shell.execute_reply": "2024-01-23T21:05:12.612065Z",
     "shell.execute_reply.started": "2024-01-23T21:05:12.574888Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_columns=[\"POS\",\"AC\",\"AN\"]\n",
    "float_columns=[\"AF\",\"K562__ref\",\"HepG2__ref\",\"SKNSH__ref\",\"K562__alt\",\"HepG2__alt\",\"SKNSH__alt\",\"K562__skew\",\"HepG2__skew\",\"SKNSH__skew\",\"cadd_phred\",\"P_ANNO\",\"mean_ref\",\"mean_skew\",\"MAF\"]\n",
    "cre_bool_columns=[]\n",
    "for column in df.columns:\n",
    "    if column.startswith(\"is_in\"):\n",
    "        cre_bool_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45c37d0-2acf-40bf-9fa1-f8dd94a74e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:12.613411Z",
     "iopub.status.busy": "2024-01-23T21:05:12.613252Z",
     "iopub.status.idle": "2024-01-23T21:05:12.631998Z",
     "shell.execute_reply": "2024-01-23T21:05:12.631513Z",
     "shell.execute_reply.started": "2024-01-23T21:05:12.613396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropna()#subset=[\"CHROM\",\"POS\",\"cadd_phred\",\"P_ANNO\",\"mean_ref\",\"mean_skew\",\"category\"]+cre_bool_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8360f33b-bd5b-4c4f-a881-5f19b5f60c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:12.633339Z",
     "iopub.status.busy": "2024-01-23T21:05:12.633001Z",
     "iopub.status.idle": "2024-01-23T21:05:12.961561Z",
     "shell.execute_reply": "2024-01-23T21:05:12.961078Z",
     "shell.execute_reply.started": "2024-01-23T21:05:12.633323Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for column in int_columns:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.IntegerType()))\n",
    "\n",
    "for column in float_columns:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.FloatType()))\n",
    "\n",
    "for column in cre_bool_columns:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.BooleanType()))\n",
    "\n",
    "    \n",
    "df_cre=df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b7229-4d25-4a65-a497-e821c5db0979",
   "metadata": {},
   "source": [
    "### Compute pleitropy\n",
    "\n",
    "\"Pleitropy\" here refers to a variant which is an emVar in multiple cell-types. We're calling emVars as anything with abs(skew)>=0.5 and max(alt activitym ref activity)>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199b1539-5171-4350-8c36-5ef08be07214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:12.962461Z",
     "iopub.status.busy": "2024-01-23T21:05:12.962304Z",
     "iopub.status.idle": "2024-01-23T21:05:13.044212Z",
     "shell.execute_reply": "2024-01-23T21:05:13.043771Z",
     "shell.execute_reply.started": "2024-01-23T21:05:12.962447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first we compute whether each variant can be called an emvar in each cell-type. \n",
    "for cell_type in [\"K562\",\"SKNSH\",\"HepG2\"]:\n",
    "    df_cre = df_cre.withColumn(f\"emVar_{cell_type}\", \n",
    "                           (F.abs(F.col(f\"{cell_type}__skew\")) >= 0.5) & \n",
    "                           (F.greatest(F.col(f\"{cell_type}__ref\"), F.col(f\"{cell_type}__alt\")) >= 1.0))\n",
    "\n",
    "#next, we count the number of cell-types each variant is an emvar in to compute the pleitropy. \n",
    "df_cre = df_cre.withColumn(\"pleio\", F.col(\"emVar_K562\").cast(\"int\") + F.col(\"emVar_SKNSH\").cast(\"int\") + F.col(\"emVar_HepG2\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25577eb5-ad89-4016-9173-2799eb0525c6",
   "metadata": {},
   "source": [
    "# compute count tables\n",
    "\n",
    "All count tables will be broken down by each of the CRE types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d08700-293b-4e32-8df0-485e291a1043",
   "metadata": {},
   "source": [
    "## PhyloP vs rarity\n",
    "- add column : \"significant\"/\"not significant\" : threshold is 2.27\n",
    "- count table of significance VS category\n",
    "- dump to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faadf1dc-21d2-4462-81d8-bb5405a49aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T23:08:17.915665Z",
     "iopub.status.busy": "2024-01-22T23:08:17.915370Z",
     "iopub.status.idle": "2024-01-22T23:08:17.932428Z",
     "shell.execute_reply": "2024-01-22T23:08:17.931993Z",
     "shell.execute_reply.started": "2024-01-22T23:08:17.915648Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_phylop_significant=df_cre.withColumn(\"phylop_significant\",F.col(\"P_ANNO\")>=2.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d9e37-b025-46f1-837b-bd5be2d916b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylop_count_table = df_phylop_significant.groupBy([\"category\",\"phylop_significant\"]+cre_bool_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff66409-7042-495f-a475-f12aee12dab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:24:16.373735Z",
     "iopub.status.busy": "2024-01-23T21:24:16.373231Z",
     "iopub.status.idle": "2024-01-23T21:24:16.375805Z",
     "shell.execute_reply": "2024-01-23T21:24:16.375436Z",
     "shell.execute_reply.started": "2024-01-23T21:24:16.373716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_base_path=\"/home/mcn26/varef/scripts/noon_data/3.count/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b8364a-69a8-481c-bbc2-43ae250f6371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T23:27:05.354324Z",
     "iopub.status.busy": "2024-01-22T23:27:05.353981Z",
     "iopub.status.idle": "2024-01-22T23:37:35.010506Z",
     "shell.execute_reply": "2024-01-22T23:37:35.010069Z",
     "shell.execute_reply.started": "2024-01-22T23:27:05.354301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ACTION\n",
    "phylop_count_table.coalesce(1).write.csv(data_base_path+\"phylop_count_table\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4191a-b438-4b36-8992-17edaf86a890",
   "metadata": {},
   "source": [
    "## PhyloP VS pleiotropy\n",
    "Are variants which cause skew in different numbers of cell-types conserved more or less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77186a24-681c-47f3-8de9-918cba2ef827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T23:08:25.244421Z",
     "iopub.status.busy": "2024-01-22T23:08:25.244105Z",
     "iopub.status.idle": "2024-01-22T23:08:25.263997Z",
     "shell.execute_reply": "2024-01-22T23:08:25.263565Z",
     "shell.execute_reply.started": "2024-01-22T23:08:25.244402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phylop_pleio = df_phylop_significant.groupBy([\"pleio\",\"phylop_significant\"]+cre_bool_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba52f96-8ae9-4439-9f26-4bd24098465c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T23:08:28.723244Z",
     "iopub.status.busy": "2024-01-22T23:08:28.722938Z",
     "iopub.status.idle": "2024-01-22T23:19:27.018758Z",
     "shell.execute_reply": "2024-01-22T23:19:27.017734Z",
     "shell.execute_reply.started": "2024-01-22T23:08:28.723225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/22 18:08:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ACTION\n",
    "phylop_pleio.coalesce(1).write.csv(data_base_path+\"phylop_pleio\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca765b-01fa-4e47-856d-a68fa5bc8e02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CADD vs rarity\n",
    "Similar approach to phylop above,\n",
    "\n",
    "Cutoffs are \n",
    "- All\n",
    "- score≥10\n",
    "- score≥20\n",
    "- score≥30\n",
    "- score≥40\n",
    "- score≥50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee94e665-9e46-4c8c-a515-6eeb66820784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T23:37:35.011759Z",
     "iopub.status.busy": "2024-01-22T23:37:35.011402Z",
     "iopub.status.idle": "2024-01-22T23:37:36.942866Z",
     "shell.execute_reply": "2024-01-22T23:37:36.942391Z",
     "shell.execute_reply.started": "2024-01-22T23:37:35.011744Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cadd_cutoff=df_cre.withColumn(\n",
    "    \"CADD>=10\",F.col(\"cadd_phred\")>=10\n",
    ").withColumn(\n",
    "    \"CADD>=20\",F.col(\"cadd_phred\")>=20\n",
    ").withColumn(\n",
    "    \"CADD>=30\",F.col(\"cadd_phred\")>=30\n",
    ").withColumn(\n",
    "    \"CADD>=40\",F.col(\"cadd_phred\")>=40\n",
    ").withColumn(\n",
    "    \"CADD>=50\",F.col(\"cadd_phred\")>=50\n",
    ")\n",
    "\n",
    "cadd_columns=[\"CADD>=10\",\"CADD>=20\",\"CADD>=30\",\"CADD>=40\",\"CADD>=50\"]\n",
    "\n",
    "with open(\"cadd_columns.pkl\",'wb') as file:\n",
    "    pickle.dump(cadd_columns,file)\n",
    "\n",
    "cadd_count_table = df_cadd_cutoff.groupBy([\"category\"]+cadd_columns+cre_bool_columns).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f10bf90-55bb-4e1e-9c7b-446acbd97d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T02:04:35.824822Z",
     "iopub.status.busy": "2024-01-23T02:04:35.824494Z",
     "iopub.status.idle": "2024-01-23T02:06:33.360645Z",
     "shell.execute_reply": "2024-01-23T02:06:33.359439Z",
     "shell.execute_reply.started": "2024-01-23T02:04:35.824803Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.         (1726 + 10) / 10743]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 15:========>                                         (1750 + 10) / 10743]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ACTION\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcadd_count_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_base_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCADD_count_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=========>                                        (1955 + 10) / 10743]\r"
     ]
    }
   ],
   "source": [
    "#ACTION\n",
    "cadd_count_table.coalesce(1).write.csv(data_base_path+\"CADD_count_table\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6badf2-6bdc-4e22-8f46-5c2bd5c2796c",
   "metadata": {},
   "source": [
    "## CADD VS pleiotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72baf1-7dc2-47a4-b543-8f932b49ffd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cadd_pleio_table = df_cadd_cutoff.groupBy([\"pleio\"]+cadd_columns+cre_bool_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe748d6-07aa-4af0-b93d-673380eb09df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ACTION\n",
    "cadd_pleio_table.coalesce(1).write.csv(data_base_path+\"CADD_pleio\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366ea82-a106-4a74-8d90-f99c58c25a1c",
   "metadata": {},
   "source": [
    "## malinouis : reference activity & skew vs rarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4da7b3-7517-4dc2-b9b9-f66babb8dd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:13.045005Z",
     "iopub.status.busy": "2024-01-23T21:05:13.044843Z",
     "iopub.status.idle": "2024-01-23T21:05:13.063032Z",
     "shell.execute_reply": "2024-01-23T21:05:13.062604Z",
     "shell.execute_reply.started": "2024-01-23T21:05:13.044990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## add a mean_alt column\n",
    "df_cre=df_cre.withColumn(\"mean_alt\", (F.col(\"K562__alt\") + F.col(\"HepG2__alt\") + F.col(\"SKNSH__alt\")) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a4839-f5e1-4d9c-9c5b-c6a89dea4fbd",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b4a270-6ed5-4469-b4b7-85e414d3375a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:14.064034Z",
     "iopub.status.busy": "2024-01-23T21:05:14.063785Z",
     "iopub.status.idle": "2024-01-23T21:05:14.069456Z",
     "shell.execute_reply": "2024-01-23T21:05:14.069091Z",
     "shell.execute_reply.started": "2024-01-23T21:05:14.064019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_column_names(var):\n",
    "    final_names=[]\n",
    "    for sub in var:\n",
    "        final_names.append(sub[0])\n",
    "    return final_names\n",
    "\n",
    "def dump_cutoff_names_to_disc(var,name):\n",
    "    #so we don't have to hard-code the names in multiple files. \n",
    "    #It's ugly enough that we're hard-coding the thresholds\n",
    "    with open(name+'.pkl', 'wb') as file:\n",
    "        final_names=get_column_names(var)\n",
    "        pickle.dump(final_names, file)\n",
    "\n",
    "#Ugly code! Really ought to combine make_reference_cutoffs & make_skew_cutoffs into one function that takes a list of intervals\n",
    "#then a second function that can make intervals based on start/stop/step\n",
    "def make_reference_cutoffs(name):\n",
    "    return [\n",
    "        [f\"{name}_(-Inf,-6)\", (F.col(name) < -6)]\n",
    "    ] + [\n",
    "        [f\"{name}_[{i},{i+1})\", (F.col(name) >= i) & (F.col(name) < i+1)] for i in range(-6, 6)\n",
    "    ] + [\n",
    "        [f\"{name}_[6,Inf)\", (F.col(name) >= 6)]\n",
    "    ]\n",
    "\n",
    "def make_skew_cutoffs(name):\n",
    "    start_int = -9   # corresponds to -4.5 (represented as -9 * 0.5)\n",
    "    end_int = 9      # corresponds to 4.5 (represented as 9 * 0.5)\n",
    "    step_int = 1     # Step of 0.5 (represented as 1 * 0.5)\n",
    "\n",
    "    return [\n",
    "        [f\"{name}_(-Inf, -4.0)\", (F.col(name) < -4.0)]\n",
    "        if i == start_int\n",
    "        else [f\"{name}_(4.0, Inf)\", (F.col(name) >= 4.0)]\n",
    "        if i == end_int - step_int\n",
    "        else [f\"{name}_[{i * 0.5:.1f}, {(i + step_int) * 0.5:.1f})\", (F.col(name) >= i * 0.5) & (F.col(name) < (i + step_int) * 0.5)]\n",
    "        for i in range(start_int, end_int, step_int)\n",
    "    ]\n",
    "\n",
    "def apply_cutoffs(df,cutoffs):\n",
    "    df_working=df\n",
    "    for name,cutoff_condition in cutoffs:\n",
    "        df_working=df_working.withColumn(name,cutoff_condition)\n",
    "    return df_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f845cf-1532-4517-9681-ebc60da5964f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:16.851993Z",
     "iopub.status.busy": "2024-01-23T21:05:16.851725Z",
     "iopub.status.idle": "2024-01-23T21:05:16.957446Z",
     "shell.execute_reply": "2024-01-23T21:05:16.957020Z",
     "shell.execute_reply.started": "2024-01-23T21:05:16.851975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list of lists of skew,ref column names we would like to use. \n",
    "cuts= [[\"mean_skew\" , \"mean_ref\"],[\"K562__skew\",\"K562__ref\"],[\"HepG2__skew\",\"HepG2__ref\"],[\"SKNSH__skew\",\"SKNSH__ref\"]]\n",
    "#create the actual cutoffs & add to the vector\n",
    "\n",
    "#cuts=[[i[0],i[1],make_skew_cutoffs(i[0]),make_reference_cutoffs(i[1])]for i in cuts]\n",
    "cuts=[{\"skew_name\":i[0],'skew_cuts':make_skew_cutoffs(i[0]),'ref_name':i[1],'ref_cuts':make_reference_cutoffs(i[1])} for i in cuts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2253ed-c15b-40ac-ac04-2b0ffca86b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:18.729427Z",
     "iopub.status.busy": "2024-01-23T21:05:18.728973Z",
     "iopub.status.idle": "2024-01-23T21:05:18.743556Z",
     "shell.execute_reply": "2024-01-23T21:05:18.743178Z",
     "shell.execute_reply.started": "2024-01-23T21:05:18.729409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dump it all to disc\n",
    "for i in cuts:\n",
    "    dump_cutoff_names_to_disc(var=i[\"skew_cuts\"],name=i[\"skew_name\"]+\".pkl\")\n",
    "    dump_cutoff_names_to_disc(var=i[\"ref_cuts\"],name=i[\"ref_name\"]+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771af1f7-f9fa-4bcd-98a1-b9283965b605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:05:20.881710Z",
     "iopub.status.busy": "2024-01-23T21:05:20.881250Z",
     "iopub.status.idle": "2024-01-23T21:05:26.227795Z",
     "shell.execute_reply": "2024-01-23T21:05:26.227343Z",
     "shell.execute_reply.started": "2024-01-23T21:05:20.881693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apply all cuts\n",
    "df_working=df_cre\n",
    "for i in cuts:\n",
    "    df_working=apply_cutoffs(df_working,i[\"skew_cuts\"])\n",
    "    df_working=apply_cutoffs(df_working,i[\"ref_cuts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15e8ab2-1436-43c6-a480-d2a1e29d677c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:15:47.620972Z",
     "iopub.status.busy": "2024-01-23T21:15:47.620702Z",
     "iopub.status.idle": "2024-01-23T21:15:58.384730Z",
     "shell.execute_reply": "2024-01-23T21:15:58.384286Z",
     "shell.execute_reply.started": "2024-01-23T21:15:47.620955Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform the counts\n",
    "#- for all cell types (separate table)\n",
    "#- for all regions of interest (within same table)\n",
    "\n",
    "tabs={}\n",
    "\n",
    "for i in cuts:\n",
    "    #get the name of the cell-type (plus mean)\n",
    "    celltype=i[\"skew_name\"].split(\"_\")[0]\n",
    "    \n",
    "    #make a big list of things we want to keep (group by)\n",
    "    #\"category\" is rarity category, \"cre_bool_columns\" are the genomic regions\n",
    "    to_group_by=[\"category\"]+cre_bool_columns\n",
    "    #then we group by skew & reference activity bins\n",
    "    to_group_by=to_group_by+get_column_names(i[\"skew_cuts\"])+get_column_names(i[\"ref_cuts\"])\n",
    "    \n",
    "    #now we have to remove commas and periods from the column names because spark will choke on them\n",
    "    renamed_column_map = {col: col.replace(',', '^').replace('.','&') for col in to_group_by}\n",
    "\n",
    "    for old_name, new_name in renamed_column_map.items():\n",
    "        df_working = df_working.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    #actually do the counting, put in 'tabs' under the cell-type name. \n",
    "    tabs[celltype]=df_working.groupBy(list(renamed_column_map.values())).count()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5a7d4-32e2-42ba-ae17-945d231fada7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T21:24:24.770356Z",
     "iopub.status.busy": "2024-01-23T21:24:24.770096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==>                                                 (494 + 10) / 12843]\r"
     ]
    }
   ],
   "source": [
    "#dump every table to disc\n",
    "for name in tabs.keys():\n",
    "    tabs[name].coalesce(1).write.csv(data_base_path+\"malinois_\"+name,mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3e25e79-3263-467a-a167-8f1bd7ad6a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:15:46.579137Z",
     "iopub.status.busy": "2024-01-23T03:15:46.578864Z",
     "iopub.status.idle": "2024-01-23T03:15:46.581531Z",
     "shell.execute_reply": "2024-01-23T03:15:46.581159Z",
     "shell.execute_reply.started": "2024-01-23T03:15:46.579120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
