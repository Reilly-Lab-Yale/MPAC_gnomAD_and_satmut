{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b616ec91-60ec-4a6b-acf0-10276eb6a069",
   "metadata": {},
   "source": [
    "This notebook annotates all gnomad variants with their corresponding malinouis predictions, the phyloP scores associated with their genomic locations, and the genomic regions (enhancer or not) they fall within. \n",
    "\n",
    "(Execute notebook after crunching wig into csv, as per other file)\n",
    "\n",
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee846e1e-6265-45e6-8b6b-1b11261fdd1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:34:30.110389Z",
     "iopub.status.busy": "2024-04-05T18:34:30.109900Z",
     "iopub.status.idle": "2024-04-05T18:34:30.777694Z",
     "shell.execute_reply": "2024-04-05T18:34:30.777190Z",
     "shell.execute_reply.started": "2024-04-05T18:34:30.110369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11021805-1df7-4ba6-99e8-28cf49efab35",
   "metadata": {},
   "source": [
    "## Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2690fc-67a5-4da4-a561-1dbb9a77a923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:34:33.850119Z",
     "iopub.status.busy": "2024-04-05T18:34:33.849673Z",
     "iopub.status.idle": "2024-04-05T18:34:40.040314Z",
     "shell.execute_reply": "2024-04-05T18:34:40.039737Z",
     "shell.execute_reply.started": "2024-04-05T18:34:33.850098Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/05 14:34:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals() and spark!=None:\n",
    "    spark.stop()\n",
    "\n",
    "    #are we running the actual script, or just testing?\n",
    "for_real=True\n",
    "\n",
    "spark=None\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea853bc-6429-40bd-a02a-c4db9b15ec4c",
   "metadata": {},
   "source": [
    "## select chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d72ca5-4edf-434b-a712-69e9bddb35a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T19:12:35.976059Z",
     "iopub.status.busy": "2024-04-05T19:12:35.975555Z",
     "iopub.status.idle": "2024-04-05T19:12:35.979006Z",
     "shell.execute_reply": "2024-04-05T19:12:35.978577Z",
     "shell.execute_reply.started": "2024-04-05T19:12:35.976041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error : did not find which chromosome we are supposed to crunch\n"
     ]
    }
   ],
   "source": [
    "chromosome=\"None\"\n",
    "\n",
    "if \"which_chr\" in os.environ:\n",
    "    chromosome=os.environ['which_chr']\n",
    "    print(\"only crunching chromosome \"+chromosome)\n",
    "else:\n",
    "    print(\"error : did not find which chromosome we are supposed to crunch\")\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59038842-6670-405b-854a-e396a81937b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T20:12:15.259216Z",
     "iopub.status.busy": "2023-12-04T20:12:15.258913Z",
     "iopub.status.idle": "2023-12-04T20:12:15.263593Z",
     "shell.execute_reply": "2023-12-04T20:12:15.263195Z",
     "shell.execute_reply.started": "2023-12-04T20:12:15.259196Z"
    },
    "tags": []
   },
   "source": [
    "## load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1494093-3c86-4637-aa8f-1acb136eec89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T19:12:37.014460Z",
     "iopub.status.busy": "2024-04-05T19:12:37.013972Z",
     "iopub.status.idle": "2024-04-05T19:12:37.036943Z",
     "shell.execute_reply": "2024-04-05T19:12:37.036464Z",
     "shell.execute_reply.started": "2024-04-05T19:12:37.014439Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the phylop tsv schema\n",
    "phylop_schema = StructType([\n",
    "    StructField(\"CHROM\", StringType(), True),\n",
    "    StructField(\"POS\", IntegerType(), True),\n",
    "    StructField(\"P_ANNO\", FloatType(), True),\n",
    "])\n",
    "\n",
    "#read in the phylop tsv\n",
    "phylop_anno = spark.read \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .schema(phylop_schema) \\\n",
    "    .csv(\"/home/mcn26/varef/scripts/noon_data/1.0.format_zoonomia_phylop/out_processed.tsv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9970c4b9-2069-407a-9ca4-781abfca2914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T19:12:45.844313Z",
     "iopub.status.busy": "2024-04-05T19:12:45.843794Z",
     "iopub.status.idle": "2024-04-05T19:12:45.919047Z",
     "shell.execute_reply": "2024-04-05T19:12:45.918592Z",
     "shell.execute_reply.started": "2024-04-05T19:12:45.844293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "vcf_schema = StructType([\n",
    "    StructField(\"CHROM\", StringType(), True),\n",
    "    StructField(\"POS\", IntegerType(), True),\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"REF\", StringType(), True),\n",
    "    StructField(\"ALT\", StringType(), True),\n",
    "    StructField(\"QUAL\", StringType(), True),\n",
    "    StructField(\"FILTER\", StringType(), True),\n",
    "    StructField(\"INFO\", StringType(), True),\n",
    "\n",
    "])\n",
    "\n",
    "vcf = spark.read \\\n",
    "    .option(\"comment\", \"#\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .schema(vcf_schema) \\\n",
    "    .csv(f\"/home/mcn26/varef/scripts/noon_data/0.merge/combined.{chromosome}.vcf.gz\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33639924-c896-402c-b65c-8696f32eb5af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T19:13:13.790951Z",
     "iopub.status.busy": "2024-04-05T19:13:13.790546Z",
     "iopub.status.idle": "2024-04-05T19:13:13.823187Z",
     "shell.execute_reply": "2024-04-05T19:13:13.822699Z",
     "shell.execute_reply.started": "2024-04-05T19:13:13.790931Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the encode enhancer dataset\n",
    "\n",
    "#define genomic region annotation (bed file) schema\n",
    "bed_schema = StructType([\n",
    "    StructField(\"CHROM\", StringType(), True),\n",
    "    StructField(\"START\", IntegerType(), True),\n",
    "    StructField(\"STOP\", StringType(), True),\n",
    "    StructField(\"misc_1\", StringType(), True),\n",
    "    StructField(\"misc_2\", StringType(), True),\n",
    "    StructField(\"REGION_TYPE\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "all_CREs=spark.read \\\n",
    "    .schema(bed_schema) \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"/home/mcn26/varef/data/ENCODE/SCREEN_v4_cCREs_agnostic/GRCh38-cCREs.V4.bed.gz\")\n",
    "\n",
    "all_CREs = all_CREs.drop(\"misc_1\", \"misc_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891727bc-f3b2-4e03-8414-bae204ad3c5f",
   "metadata": {},
   "source": [
    "## Split relevant information from the VCF info field\n",
    "...into its own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e899f207-7cc0-467b-8f7e-92f2216b1457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T21:16:17.858091Z",
     "iopub.status.busy": "2023-12-19T21:16:17.857609Z",
     "iopub.status.idle": "2023-12-19T21:16:18.155606Z",
     "shell.execute_reply": "2023-12-19T21:16:18.155106Z",
     "shell.execute_reply.started": "2023-12-19T21:16:17.858071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####The `INFO` field contains a lot of useful information, but it is all smashed together into a string. \n",
    "#Let's extract information from that string. \n",
    "\n",
    "keys_to_extract = [#NONE CAN BE SUBSTRINGS OF THE OTHERS\n",
    "    \"K562__ref\", \"HepG2__ref\", \"SKNSH__ref\", \"K562__alt\", \"HepG2__alt\", \"SKNSH__alt\",\n",
    "    \"K562__skew\", \"HepG2__skew\", \"SKNSH__skew\", \"AC\", \"AN\", \"AF\", \"cadd_phred\",# \"P_ANNO\" already in its own column\n",
    "]\n",
    "\n",
    "# Apply the regexp_extract function to the DataFrame to create new columns for each key.\n",
    "# The expression '([^;]*)' captures any sequence of characters that are not a semicolon,\n",
    "# which is assumed to be the delimiter for the key-value pairs in the 'INFO' column.\n",
    "\n",
    "for key in keys_to_extract:\n",
    "\n",
    "    #df = df.withColumn(key, regexp_extract(col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1))\n",
    "    #when we find something put it, whne we don't put None\n",
    "    vcf = vcf.withColumn(key, \n",
    "                       F.when(\n",
    "                           F.regexp_extract(F.col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1) != \"\",\n",
    "                           F.regexp_extract(F.col(\"INFO\"), \"{}=([^;]+);?\".format(key), 1)).otherwise(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab95dd-2e3f-4a5a-8973-eb242cdf3f29",
   "metadata": {},
   "source": [
    "## Filter out the gorp\n",
    "\n",
    "- Filter out the gorp\n",
    "    - Remove low-quality variants that don't pass GNOMAD's own filters.\n",
    "    - Remove low-quality variants not queried in a large number of individuals\n",
    "    - Remove variants with a MAF of 0 (they don't really \"vary\") in the population if they dont exist.\n",
    "    - Remove those variants that don't have all of the relevant metrics.\n",
    "    - Remove non-SNP variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f2af8f-c370-4f41-a571-9748c9cdf476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T21:16:20.371234Z",
     "iopub.status.busy": "2023-12-19T21:16:20.370723Z",
     "iopub.status.idle": "2023-12-19T21:16:20.398236Z",
     "shell.execute_reply": "2023-12-19T21:16:20.397692Z",
     "shell.execute_reply.started": "2023-12-19T21:16:20.371216Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vcf = vcf.filter(\n",
    "    #make sure we have the necessary population stats\n",
    "    (F.col(\"AF\").isNotNull()) & \n",
    "    (F.col(\"AC\").isNotNull()) & \n",
    "    (F.col(\"AN\").isNotNull()) & \n",
    "    \n",
    "    #make sure we a cadd_phred score\n",
    "    (F.col(\"cadd_phred\").isNotNull()) & \n",
    "\n",
    "    #check variant has been queried in a reasonably large number of people\n",
    "    #same as gnomad's own warning threshold\n",
    "    #see readme\n",
    "    (F.col(\"AN\").cast(\"int\") >= 76156) &\n",
    "    \n",
    "    #gnomad filters passed. See original gnomad vcf header for spec.\n",
    "    (F.col(\"FILTER\") == \"PASS\") \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc577c9-6f52-4942-a342-1f465285745d",
   "metadata": {},
   "source": [
    "## Add the genomic region annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c98be64d-19f3-4925-a49a-6145a3d2e96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T18:55:50.400363Z",
     "iopub.status.busy": "2023-12-18T18:55:50.400072Z",
     "iopub.status.idle": "2023-12-18T18:55:50.404177Z",
     "shell.execute_reply": "2023-12-18T18:55:50.403766Z",
     "shell.execute_reply.started": "2023-12-18T18:55:50.400345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_genomic_annotation(loci,regions,name):\n",
    "    \n",
    "\n",
    "    # as usual, we have to worry about coordinate systems\n",
    "    # VCFs are 1-based\n",
    "    # BEDs are 0-based\n",
    "    # Chr1        T   A   C   G   T\n",
    "    #           | | | | | | | | | |\n",
    "    # 1 based   | 1 | 2 | 3 | 4 | 5\n",
    "    # 0 based   0   1   2   3   4\n",
    "\n",
    "    #subset the provided regions to only those of the region of interest\n",
    "    regions_subset = regions.filter( (F.col(\"REGION_TYPE\") == name ) )\n",
    "    \n",
    "    #joins \n",
    "    result = loci.join(\n",
    "        regions_subset,\n",
    "        (loci.CHROM == regions_subset.CHROM) & \n",
    "        (loci.POS > regions_subset.START) & \n",
    "        (loci.POS <= regions_subset.STOP),\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # note the potential for a bug here. Left join can perform one-to-many\n",
    "    # and will create multiple entries. That is, if a variant falls inside\n",
    "    # multiple overlapping regions of the same region type, the variant will be duplicated. \n",
    "    # However, as I've checked in helper/check_overlap/, this should not be a problem\n",
    "    # for this particular dataset\n",
    "    \n",
    "    # Add a boolean column 'is_in_region'\n",
    "    result = result.withColumn(\"is_in_\"+name, F.col(\"start\").isNotNull())\n",
    "\n",
    "    # Select only columns from df_loci and the new boolean column\n",
    "    final_result = result.select(loci[\"*\"], \"is_in_\"+name)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9c7021-9298-43ea-bbbe-21c195620c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T21:16:31.862723Z",
     "iopub.status.busy": "2023-12-19T21:16:31.862438Z",
     "iopub.status.idle": "2023-12-19T21:16:34.600988Z",
     "shell.execute_reply": "2023-12-19T21:16:34.600466Z",
     "shell.execute_reply.started": "2023-12-19T21:16:31.862706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dELS', 'CA', 'pELS', 'CA-H3K4me3', 'CA-CTCF', 'PLS', 'TF', 'CA-TF']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#get all the unique values we might want to add\n",
    "unique_values=all_CREs.select(\"REGION_TYPE\").distinct()\n",
    "\n",
    "unique_list = [row['REGION_TYPE'] for row in unique_values.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cfdf434-7a36-45f0-baae-92389f0a5ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T18:58:17.857271Z",
     "iopub.status.busy": "2023-12-18T18:58:17.856861Z",
     "iopub.status.idle": "2023-12-18T18:58:18.188213Z",
     "shell.execute_reply": "2023-12-18T18:58:18.187747Z",
     "shell.execute_reply.started": "2023-12-18T18:58:17.857251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make a boolean column for each genomic region annotation\n",
    "genomic_regions_added=vcf\n",
    "for region in unique_list:\n",
    "    genomic_regions_added=add_genomic_annotation(loci=genomic_regions_added,regions=all_CREs,name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a8b8c-c85d-4bcc-806a-74d941566215",
   "metadata": {},
   "source": [
    "## Add the phyloP annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ebf30b3-0042-4a9d-a028-236ef1261fe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T18:43:29.755022Z",
     "iopub.status.busy": "2023-12-06T18:43:29.754544Z",
     "iopub.status.idle": "2023-12-06T18:43:29.765893Z",
     "shell.execute_reply": "2023-12-06T18:43:29.765467Z",
     "shell.execute_reply.started": "2023-12-06T18:43:29.755005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phyloP_annotated = genomic_regions_added.join(phylop_anno, on=[\"CHROM\", \"POS\"], how=\"inner\")\n",
    "#inner join means we'll drop all those variants without scores..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122d4af-2587-42af-8c3c-5f3b729f3067",
   "metadata": {},
   "source": [
    "Compute mean malinouis reference activity and mean malinouis skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30db1f0c-7283-47f4-aa8f-8f9d3a1302de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T18:43:31.306668Z",
     "iopub.status.busy": "2023-12-06T18:43:31.306278Z",
     "iopub.status.idle": "2023-12-06T18:43:31.363347Z",
     "shell.execute_reply": "2023-12-06T18:43:31.362808Z",
     "shell.execute_reply.started": "2023-12-06T18:43:31.306650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reference activity\n",
    "\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"K562__ref\", F.col(\"K562__ref\").cast(\"float\"))\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"HepG2__ref\", F.col(\"HepG2__ref\").cast(\"float\"))\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"SKNSH__ref\", F.col(\"SKNSH__ref\").cast(\"float\"))\n",
    "\n",
    "phyloP_annotated=phyloP_annotated.withColumn(\"mean_ref\", (F.col(\"K562__ref\") + F.col(\"HepG2__ref\") + F.col(\"SKNSH__ref\")) / 3)\n",
    "\n",
    "#skew\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"K562__skew\", F.col(\"K562__skew\").cast(\"float\"))\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"HepG2__skew\", F.col(\"HepG2__skew\").cast(\"float\"))\n",
    "phyloP_annotated = phyloP_annotated.withColumn(\"SKNSH__skew\", F.col(\"SKNSH__skew\").cast(\"float\"))\n",
    "\n",
    "phyloP_annotated=phyloP_annotated.withColumn(\"mean_skew\", (F.col(\"K562__skew\") + F.col(\"HepG2__skew\") + F.col(\"SKNSH__skew\")) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e3a92-4957-4726-8035-2a03d7148ad6",
   "metadata": {},
   "source": [
    "## Add rarity codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e421adf-7bd0-439a-8b27-5c2da6102475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T18:43:32.498000Z",
     "iopub.status.busy": "2023-12-06T18:43:32.497510Z",
     "iopub.status.idle": "2023-12-06T18:43:32.536614Z",
     "shell.execute_reply": "2023-12-06T18:43:32.536169Z",
     "shell.execute_reply.started": "2023-12-06T18:43:32.497982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, convert 'AC' and 'AF' to integer and float types respectively using withColumn\n",
    "df = phyloP_annotated.withColumn(\"AC\", F.col(\"AC\").cast(\"int\"))\n",
    "df = df.withColumn(\"AF\", F.col(\"AF\").cast(\"float\"))\n",
    "\n",
    "# Calculate MAF using the Spark SQL functions\n",
    "df = df.withColumn(\"MAF\", F.least(F.col(\"AF\"), 1 - F.col(\"AF\")))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"category\", \n",
    "                   F.when((F.col(\"MAF\") == 0.0) | (F.col(\"AC\") == 0), \"MAF_OR_AC_IS_ZERO\")\n",
    "                    .when(F.col(\"AC\") == 1, \"SINGLETON\")\n",
    "                    .when(F.col(\"MAF\") < 0.01 / 100, \"ULTRARARE\")\n",
    "                    .when(F.col(\"MAF\") < 0.1 / 100, \"RARE\")\n",
    "                    .when(F.col(\"MAF\") < 1 / 100, \"LOW_FREQ\")\n",
    "                    .otherwise(\"COMMON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "423e7147-33ab-40f2-bcc0-9ed86050f6b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T18:43:33.971254Z",
     "iopub.status.busy": "2023-12-06T18:43:33.970869Z",
     "iopub.status.idle": "2023-12-06T18:43:39.189052Z",
     "shell.execute_reply": "2023-12-06T18:43:39.187888Z",
     "shell.execute_reply.started": "2023-12-06T18:43:33.971235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/06 13:43:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 5) / 444]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mcn26/varef/scripts/noon_data/1.annotate/annotated_output.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mcn_vareff/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                (5 + 6) / 444][Stage 4:>                 (0 + 0) / 22]\r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(f\"/home/mcn26/varef/scripts/noon_data/2.0.annotate/annotated_output_{chromosome}.csv.gz\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
