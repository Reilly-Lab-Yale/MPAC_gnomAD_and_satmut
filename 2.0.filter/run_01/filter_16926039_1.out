only crunching chromosome chr1
Traceback (most recent call last):
  File "/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_scripts/2.filter/2.filter.py", line 106, in <module>
    non_exonic.write.csv(f"/home/mcn26/varef/scripts/noon_data/2.filter/filtered_output_{chromosome}.csv", header=True, mode="overwrite")
  File "/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1864, in csv
  File "/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/home/mcn26/.conda/envs/mcn_vareff/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o61.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 803 in stage 7.0 failed 1 times, most recent failure: Lost task 803.0 in stage 7.0 (TID 2217) (r814u03n02.mccleary.ycrc.yale.edu executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/mcn26/varef/scripts/noon_data/2.filter/filtered_output_chr1.csv.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
Internal state when error was thrown: recordCount=34567, recordData=[chr1, 112318654, rs1570672985, A, T, ., PASS, K562__ref=0.16556011;HepG2__ref=0.23341496;SKNSH__ref=0.26695815;K562__alt=0.18004486;HepG2__alt=0.25318825;SKNSH__alt=0.30630174;K562__skew=0.014484773;HepG2__skew=0.019773245;SKNSH__skew=0.039343584;AN_oth=2090;AN_ami=912;AN_sas=4830;AN_fin=10622;AN_eas=5200;AN_amr=15278;AN_afr=41446;AN_mid=316;AN_asj=3468;AN_nfe=68036;cadd_raw_score=-0.63877;cadd_phred=0.06;vep=T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000427290|lncRNA||1/1|ENST00000427290.1:n.167+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808||3|||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000654472|lncRNA||1/2|ENST00000654472.1:n.145+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000658120|lncRNA||1/1|ENST00000658120.1:n.256+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000659549|lncRNA||1/2|ENST00000659549.1:n.264+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|YES||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000663553|lncRNA||1/1|ENST00000663553.1:n.205+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LOC105378909|105378909|Transcript|XR_001738189.1|lncRNA||1/1|XR_001738189.1:n.317+41708T>A|||||||1||-1|SNV||EntrezGene||YES||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LOC105378909|105378909|Transcript|XR_947710.1|lncRNA||1/2|XR_947710.1:n.272+41708T>A|||||||1||-1|SNV||EntrezGene||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00000368929|promoter_flanking_region||||||||||1|||SNV||||||||||||||||||||||||;AC=1;AN=152198;AF=6.57039e-06;AC_oth=0;AF_oth=0;AC_ami=0;AF_ami=0;AC_sas=0;AF_sas=0;AC_fin=0;AF_fin=0;AC_eas=0;AF_eas=0;AC_amr=0;AF_amr=0;AC_afr=1;AF_afr=2.41278e-05;AC_mid=0;AF_mid=0;AC_asj=0;AF_asj=0;AC_nfe=0;AF_nfe=0;AC_popmax=1;AN_popmax=41446;AF_popmax=2.41278e-05, 0.16556011, 0.23341496, 0.26695815, 0.18004486, 0.25318825, 0.30630174, 0.014484773, 0.019773245, 0.039343584, 1, 152198, 6.57039E-6, 0.06, false, false, false, false, false, false, false, false, 1.106, 0.2219777504603068, 0.024533867835998535, 6.57039E-6, SINGLETON]
	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more
Caused by: java.lang.IllegalStateException: Error closing the output.
	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
	... 25 more
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Disk quota exceeded
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:349)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)
	at java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:206)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:113)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:459)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:218)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:165)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:447)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:443)
	at java.base/sun.nio.cs.StreamEncoder.lockedClose(StreamEncoder.java:241)
	at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:226)
	at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:267)
	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)
	... 28 more
Caused by: java.io.IOException: Disk quota exceeded
	at java.base/java.io.FileOutputStream.writeBytes(Native Method)
	at java.base/java.io.FileOutputStream.write(FileOutputStream.java:373)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:345)
	... 45 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:578)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/mcn26/varef/scripts/noon_data/2.filter/filtered_output_chr1.csv.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more
Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
Internal state when error was thrown: recordCount=34567, recordData=[chr1, 112318654, rs1570672985, A, T, ., PASS, K562__ref=0.16556011;HepG2__ref=0.23341496;SKNSH__ref=0.26695815;K562__alt=0.18004486;HepG2__alt=0.25318825;SKNSH__alt=0.30630174;K562__skew=0.014484773;HepG2__skew=0.019773245;SKNSH__skew=0.039343584;AN_oth=2090;AN_ami=912;AN_sas=4830;AN_fin=10622;AN_eas=5200;AN_amr=15278;AN_afr=41446;AN_mid=316;AN_asj=3468;AN_nfe=68036;cadd_raw_score=-0.63877;cadd_phred=0.06;vep=T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000427290|lncRNA||1/1|ENST00000427290.1:n.167+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808||3|||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000654472|lncRNA||1/2|ENST00000654472.1:n.145+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000658120|lncRNA||1/1|ENST00000658120.1:n.256+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000659549|lncRNA||1/2|ENST00000659549.1:n.264+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|YES||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LINC02884|ENSG00000231246|Transcript|ENST00000663553|lncRNA||1/1|ENST00000663553.1:n.205+41708T>A|||||||1||-1|SNV||HGNC|HGNC:54808|||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LOC105378909|105378909|Transcript|XR_001738189.1|lncRNA||1/1|XR_001738189.1:n.317+41708T>A|||||||1||-1|SNV||EntrezGene||YES||||||||||||||||||||,T|intron_variant&non_coding_transcript_variant|MODIFIER|LOC105378909|105378909|Transcript|XR_947710.1|lncRNA||1/2|XR_947710.1:n.272+41708T>A|||||||1||-1|SNV||EntrezGene||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00000368929|promoter_flanking_region||||||||||1|||SNV||||||||||||||||||||||||;AC=1;AN=152198;AF=6.57039e-06;AC_oth=0;AF_oth=0;AC_ami=0;AF_ami=0;AC_sas=0;AF_sas=0;AC_fin=0;AF_fin=0;AC_eas=0;AF_eas=0;AC_amr=0;AF_amr=0;AC_afr=1;AF_afr=2.41278e-05;AC_mid=0;AF_mid=0;AC_asj=0;AF_asj=0;AC_nfe=0;AF_nfe=0;AC_popmax=1;AN_popmax=41446;AF_popmax=2.41278e-05, 0.16556011, 0.23341496, 0.26695815, 0.18004486, 0.25318825, 0.30630174, 0.014484773, 0.019773245, 0.039343584, 1, 152198, 6.57039E-6, 0.06, false, false, false, false, false, false, false, false, 1.106, 0.2219777504603068, 0.024533867835998535, 6.57039E-6, SINGLETON]
	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 17 more
Caused by: java.lang.IllegalStateException: Error closing the output.
	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
	... 25 more
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Disk quota exceeded
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:349)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)
	at java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)
	at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:206)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62)
	at java.base/java.io.DataOutputStream.write(DataOutputStream.java:113)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:459)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:218)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:165)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:146)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:447)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:443)
	at java.base/sun.nio.cs.StreamEncoder.lockedClose(StreamEncoder.java:241)
	at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:226)
	at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:267)
	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)
	... 28 more
Caused by: java.io.IOException: Disk quota exceeded
	at java.base/java.io.FileOutputStream.writeBytes(Native Method)
	at java.base/java.io.FileOutputStream.write(FileOutputStream.java:373)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:345)
	... 45 more

