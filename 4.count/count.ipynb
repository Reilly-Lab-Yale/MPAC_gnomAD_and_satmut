{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5941730-7061-4b4f-bf36-4615d23f1d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will generate a number of count tables, described in sections below.\n",
    "\n",
    "The basic approach is to load the data, then create a bunch of boolean columns corresponding to the various **conditions** (including bins) we would like to count. Then we compute a count table, where each row is a different combination of these boolean values. \n",
    "\n",
    "This results in counts of many rows counting combinations of categories we do not care about. So we subsequently groupby+sum to create sub-count-tables counting combinations of criteria we think may be meaningful.\n",
    "\n",
    "At various points, we pickle & dump to disc lists of threshold criteria, for the downstream graphing to use. (This allows changes in criteria to be quickly passed to the graphing scripts).\n",
    "\n",
    "(the old approach was more ad-hoc, re-doing the counting for each meaningful set of criteria. This caused redundant computation & was less flexible to adding more sets.)\n",
    "\n",
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a51615e-a596-45e9-a1db-b3877628a551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:03.732366Z",
     "iopub.status.busy": "2024-04-16T22:47:03.731872Z",
     "iopub.status.idle": "2024-04-16T22:47:03.737568Z",
     "shell.execute_reply": "2024-04-16T22:47:03.737141Z",
     "shell.execute_reply.started": "2024-04-16T22:47:03.732338Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_output_base=\"/home/mcn26/varef/scripts/noon_data/4.count/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d4983-60fa-4ac1-9c00-4373c4fc3672",
   "metadata": {
    "tags": []
   },
   "source": [
    "## import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0363d27-c5f7-4925-aa1f-e911f503be57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:04.693894Z",
     "iopub.status.busy": "2024-04-16T22:47:04.693349Z",
     "iopub.status.idle": "2024-04-16T22:47:05.526168Z",
     "shell.execute_reply": "2024-04-16T22:47:05.525706Z",
     "shell.execute_reply.started": "2024-04-16T22:47:04.693875Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67873089-a7f3-4ccf-8403-8a2413d84479",
   "metadata": {
    "tags": []
   },
   "source": [
    "## create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f220458-459e-4603-834f-9ee2fb2598cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:06.123817Z",
     "iopub.status.busy": "2024-04-16T22:47:06.123180Z",
     "iopub.status.idle": "2024-04-16T22:47:08.346341Z",
     "shell.execute_reply": "2024-04-16T22:47:08.345748Z",
     "shell.execute_reply.started": "2024-04-16T22:47:06.123796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/16 18:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Count\")\\\n",
    "\n",
    "# Create a SparkContext with the specified configurations\n",
    "if 'spark' in locals() and spark!=None:\n",
    "    spark.stop()\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a SparkSession from the SparkContext\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fcdbc-c7e1-467f-ad0a-c299ce85cda4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in gnomad variants filtered in the last script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0c646e-25e9-4d80-901f-f33e83cc7f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:08.347669Z",
     "iopub.status.busy": "2024-04-16T22:47:08.347422Z",
     "iopub.status.idle": "2024-04-16T22:47:20.304299Z",
     "shell.execute_reply": "2024-04-16T22:47:20.303790Z",
     "shell.execute_reply.started": "2024-04-16T22:47:08.347652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#loading in all autosomes\n",
    "#Skipping sex chromosomes, see readme\n",
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"/home/mcn26/varef/scripts/noon_data/3.5add_TF_footprints/chr*/*.csv.gz\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821e580-d5eb-43ab-960a-5ded4f2fca68",
   "metadata": {},
   "source": [
    "## cast columns to the appropriate types & Drop columns rows with null values. \n",
    "\n",
    "We could only drop those rows with null malinouis skew when computing malinouis-skew-based metrics, drop rows with no phyloP scores when computing phyloP-based metrics, etc etc. However, this would result in different sets of variants summarized by each graph, which could create biases : if, for example, PhyloP scores are annotated for a nonrandom set of variants. Therefore I will drop rows with null data in any relevant columns prior to subsequent analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dd9f2e-712c-4e74-9a01-3969e82de5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.305607Z",
     "iopub.status.busy": "2024-04-16T22:47:20.305251Z",
     "iopub.status.idle": "2024-04-16T22:47:20.318991Z",
     "shell.execute_reply": "2024-04-16T22:47:20.318615Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.305590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_columns=[\"POS\",\"AC\",\"AN\",\"pleio\"]\n",
    "float_columns=[\"AF\",\"K562__ref\",\"HepG2__ref\",\"SKNSH__ref\",\"K562__alt\",\"HepG2__alt\",\"SKNSH__alt\",\"K562__skew\",\"HepG2__skew\",\"SKNSH__skew\",\"cadd_phred\",\"P_ANNO\",\"mean_ref\",\"mean_skew\",\"MAF\"]\n",
    "cre_bool_columns=[]\n",
    "for column in df.columns:\n",
    "    if column.startswith(\"is_in\"):\n",
    "        cre_bool_columns.append(column)\n",
    "        \n",
    "emvar_bool_columns=[\"emVar_K562\",\"emVar_SKNSH\",\"emVar_HepG2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8360f33b-bd5b-4c4f-a881-5f19b5f60c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.319731Z",
     "iopub.status.busy": "2024-04-16T22:47:20.319589Z",
     "iopub.status.idle": "2024-04-16T22:47:20.691380Z",
     "shell.execute_reply": "2024-04-16T22:47:20.690922Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.319717Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for column in int_columns:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.IntegerType()))\n",
    "\n",
    "for column in float_columns:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.FloatType()))\n",
    "\n",
    "for column in cre_bool_columns+emvar_bool_columns+[\"in_TF\"]:\n",
    "    df = df.withColumn(column, F.col(column).cast(T.BooleanType()))\n",
    "\n",
    "    \n",
    "df_cre=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45c37d0-2acf-40bf-9fa1-f8dd94a74e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.692757Z",
     "iopub.status.busy": "2024-04-16T22:47:20.692486Z",
     "iopub.status.idle": "2024-04-16T22:47:20.706847Z",
     "shell.execute_reply": "2024-04-16T22:47:20.706449Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.692740Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cre = df_cre.dropna(subset=int_columns+float_columns+cre_bool_columns+emvar_bool_columns+[\"in_TF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761c8b8-9723-4e81-8e87-fb4ac2f732c6",
   "metadata": {},
   "source": [
    "# Add conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5b483-ae9f-4b49-abbb-cb1fa1e0a006",
   "metadata": {},
   "source": [
    "## Phylop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f9932f-44ca-4ab1-8f62-2fea1142909a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.707548Z",
     "iopub.status.busy": "2024-04-16T22:47:20.707405Z",
     "iopub.status.idle": "2024-04-16T22:47:20.737113Z",
     "shell.execute_reply": "2024-04-16T22:47:20.736678Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.707533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cre=df_cre.withColumn(\"phylop_significant\",F.col(\"P_ANNO\")>=2.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be9ca6-7204-40fc-8e50-65b14748273b",
   "metadata": {},
   "source": [
    "## CADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88759813-5dab-4c04-bdba-fa6a30c22242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.738076Z",
     "iopub.status.busy": "2024-04-16T22:47:20.737838Z",
     "iopub.status.idle": "2024-04-16T22:47:20.810853Z",
     "shell.execute_reply": "2024-04-16T22:47:20.810388Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.738058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cre=df_cre.withColumn(\n",
    "    \"CADD>=10\",F.col(\"cadd_phred\")>=10\n",
    ").withColumn(\n",
    "    \"CADD>=20\",F.col(\"cadd_phred\")>=20\n",
    ").withColumn(\n",
    "    \"CADD>=30\",F.col(\"cadd_phred\")>=30\n",
    ").withColumn(\n",
    "    \"CADD>=40\",F.col(\"cadd_phred\")>=40\n",
    ").withColumn(\n",
    "    \"CADD>=50\",F.col(\"cadd_phred\")>=50\n",
    ")\n",
    "\n",
    "cadd_columns=[\"CADD>=10\",\"CADD>=20\",\"CADD>=30\",\"CADD>=40\",\"CADD>=50\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027d9182-1c89-4dc4-982f-5cfb4d41f9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.811843Z",
     "iopub.status.busy": "2024-04-16T22:47:20.811547Z",
     "iopub.status.idle": "2024-04-16T22:47:20.814098Z",
     "shell.execute_reply": "2024-04-16T22:47:20.813763Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.811827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"cadd_columns.pkl\",'wb') as file:\n",
    "    pickle.dump(cadd_columns,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dace43-18f0-454f-b42d-46ddc2ffc1f6",
   "metadata": {},
   "source": [
    "## malinois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579c940-2963-4281-8018-e7d9137f2e70",
   "metadata": {},
   "source": [
    "Add a mean column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52da7675-551a-4830-b19b-cdf630660929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.814674Z",
     "iopub.status.busy": "2024-04-16T22:47:20.814541Z",
     "iopub.status.idle": "2024-04-16T22:47:20.858840Z",
     "shell.execute_reply": "2024-04-16T22:47:20.858430Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.814662Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cre=df_cre.withColumn(\"mean_alt\", (F.col(\"K562__alt\") + F.col(\"HepG2__alt\") + F.col(\"SKNSH__alt\")) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fa58a-b49c-4010-92df-4dabc55407dc",
   "metadata": {},
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ed7f37-e312-48b5-9f17-54c8bbb045fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.859772Z",
     "iopub.status.busy": "2024-04-16T22:47:20.859428Z",
     "iopub.status.idle": "2024-04-16T22:47:20.867580Z",
     "shell.execute_reply": "2024-04-16T22:47:20.867228Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.859758Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_column_names(var):\n",
    "    final_names=[]\n",
    "    for sub in var:\n",
    "        final_names.append(sub[0])\n",
    "    return final_names\n",
    "\n",
    "def dump_cutoff_names_to_disc(var,name):\n",
    "    #so we don't have to hard-code the names in multiple files. \n",
    "    #It's ugly enough that we're hard-coding the thresholds\n",
    "    with open(name+'.pkl', 'wb') as file:\n",
    "        final_names=get_column_names(var)\n",
    "        pickle.dump(final_names, file)\n",
    "\n",
    "#Ugly code! Really ought to combine make_reference_cutoffs & make_skew_cutoffs into one function that takes a list of intervals\n",
    "#then a second function that can make intervals based on start/stop/step\n",
    "def make_reference_cutoffs(name):\n",
    "    return [\n",
    "        [f\"{name}_(-Inf,-1)\", (F.col(name) < -1)]\n",
    "    ] + [\n",
    "        [f\"{name}_[{i},{i+1})\", (F.col(name) >= i) & (F.col(name) < i+1)] for i in range(-1, 5)\n",
    "    ] + [\n",
    "        [f\"{name}_[5,Inf)\", (F.col(name) >= 5)]\n",
    "    ]\n",
    "\n",
    "def make_skew_cutoffs(name):\n",
    "    \n",
    "    \n",
    "    #skew: c(-Inf, -1.5, -1, -0.5, -0.2, -0.05, -0.02, 0, 0.02, 0.05, 0.2, 0.5, 1, 1.5, Inf)\n",
    "    \n",
    "    return [\n",
    "        [f\"{name}_(-Inf, -1.5)\", (F.col(name) < -1.5)],\n",
    "        [f\"{name}_[-1.5, -1.0)\", (F.col(name) >= -1.5) & (F.col(name) < -1.0)],\n",
    "        [f\"{name}_[-1.0, -0.5)\", (F.col(name) >= -1.0) & (F.col(name) < -0.5)],\n",
    "        [f\"{name}_[-0.5, -0.2)\", (F.col(name) >= -0.5) & (F.col(name) < -0.2)],\n",
    "        [f\"{name}_[-0.2, -0.05)\", (F.col(name) >= -0.2) & (F.col(name) < -0.05)],\n",
    "        [f\"{name}_[-0.05, -0.02)\", (F.col(name) >= -0.05) & (F.col(name) < -0.02)],\n",
    "        [f\"{name}_[-0.02, 0)\", (F.col(name) >= -0.02) & (F.col(name) < 0)],\n",
    "        [f\"{name}_[0, 0.02)\", (F.col(name) >= 0) & (F.col(name) < 0.02)],\n",
    "        [f\"{name}_[0.02, 0.05)\", (F.col(name) >= 0.02) & (F.col(name) < 0.05)],\n",
    "        [f\"{name}_[0.05, 0.2)\", (F.col(name) >= 0.05) & (F.col(name) < 0.2)],\n",
    "        [f\"{name}_[0.2, 0.5)\", (F.col(name) >= 0.2) & (F.col(name) < 0.5)],\n",
    "        [f\"{name}_[0.5, 1.0)\", (F.col(name) >= 0.5) & (F.col(name) < 1.0)],\n",
    "        [f\"{name}_[1.0, 1.5)\", (F.col(name) >= 1.0) & (F.col(name) < 1.5)],\n",
    "        [f\"{name}_(1.5, Inf)\", (F.col(name) > 1.5)],\n",
    "        \n",
    "    ]\n",
    "\n",
    "    #return [\n",
    "    #    \n",
    "    #    if i == start_int\n",
    "    #    else [f\"{name}_(1.5, Inf)\", (F.col(name) >= 4.0)]\n",
    "    #    if i == end_int - step_int\n",
    "    #    else [f\"{name}_[{i * 0.5:.1f}, {(i + step_int) * 0.5:.1f})\", (F.col(name) >= i * 0.5) & (F.col(name) < (i + step_int) * 0.5)]\n",
    "    #    for i in range(start_int, end_int, step_int)\n",
    "    #]\n",
    "\n",
    "def apply_cutoffs(df,cutoffs):\n",
    "    df_working=df\n",
    "    for name,cutoff_condition in cutoffs:\n",
    "        df_working=df_working.withColumn(name,cutoff_condition)\n",
    "    return df_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6ecbd7-e9d8-4ff4-b441-d8449ea73752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:20.868804Z",
     "iopub.status.busy": "2024-04-16T22:47:20.868585Z",
     "iopub.status.idle": "2024-04-16T22:47:20.894024Z",
     "shell.execute_reply": "2024-04-16T22:47:20.893675Z",
     "shell.execute_reply.started": "2024-04-16T22:47:20.868791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abc_(-Inf, -1.5)', Column<'(abc < -1.5)'>], ['abc_[-1.5, -1.0)', Column<'((abc >= -1.5) AND (abc < -1.0))'>], ['abc_[-1.0, -0.5)', Column<'((abc >= -1.0) AND (abc < -0.5))'>], ['abc_[-0.5, -0.2)', Column<'((abc >= -0.5) AND (abc < -0.2))'>], ['abc_[-0.2, -0.05)', Column<'((abc >= -0.2) AND (abc < -0.05))'>], ['abc_[-0.05, -0.02)', Column<'((abc >= -0.05) AND (abc < -0.02))'>], ['abc_[-0.02, 0)', Column<'((abc >= -0.02) AND (abc < 0))'>], ['abc_[0, 0.02)', Column<'((abc >= 0) AND (abc < 0.02))'>], ['abc_[0.02, 0.05)', Column<'((abc >= 0.02) AND (abc < 0.05))'>], ['abc_[0.05, 0.2)', Column<'((abc >= 0.05) AND (abc < 0.2))'>], ['abc_[0.2, 0.5)', Column<'((abc >= 0.2) AND (abc < 0.5))'>], ['abc_[0.5, 1.0)', Column<'((abc >= 0.5) AND (abc < 1.0))'>], ['abc_[1.0, 1.5)', Column<'((abc >= 1.0) AND (abc < 1.5))'>], ['abc_(1.5, Inf)', Column<'(abc > 1.5)'>]]\n"
     ]
    }
   ],
   "source": [
    "print(make_skew_cutoffs(\"abc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd062b3-3b92-418c-8b28-12bb688a4736",
   "metadata": {},
   "source": [
    "Create the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261af8c1-5c36-4944-8f05-d9977450bab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:21.216114Z",
     "iopub.status.busy": "2024-04-16T22:47:21.215754Z",
     "iopub.status.idle": "2024-04-16T22:47:21.289647Z",
     "shell.execute_reply": "2024-04-16T22:47:21.289198Z",
     "shell.execute_reply.started": "2024-04-16T22:47:21.216096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/16 18:47:22 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "#list of lists of skew,ref column names we would like to use. \n",
    "cuts= [[\"mean_skew\" , \"mean_ref\"],[\"K562__skew\",\"K562__ref\"],[\"HepG2__skew\",\"HepG2__ref\"],[\"SKNSH__skew\",\"SKNSH__ref\"]]\n",
    "\n",
    "#create the actual cutoffs & add to the vector\n",
    "cuts=[{\"skew_name\":i[0],'skew_cuts':make_skew_cutoffs(i[0]),'ref_name':i[1],'ref_cuts':make_reference_cutoffs(i[1])} for i in cuts]\n",
    "#dump it all to disc\n",
    "for i in cuts:\n",
    "    dump_cutoff_names_to_disc(var=i[\"skew_cuts\"],name=i[\"skew_name\"]+\".pkl\")\n",
    "    dump_cutoff_names_to_disc(var=i[\"ref_cuts\"],name=i[\"ref_name\"]+\".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796b3cf-fe10-4597-8c45-a35d81c6c997",
   "metadata": {},
   "source": [
    "apply all cuts & save their names for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d13b1c-ddaa-4b36-ac36-3b93692efd0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:24.209971Z",
     "iopub.status.busy": "2024-04-16T22:47:24.209663Z",
     "iopub.status.idle": "2024-04-16T22:47:27.210492Z",
     "shell.execute_reply": "2024-04-16T22:47:27.210007Z",
     "shell.execute_reply.started": "2024-04-16T22:47:24.209952Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_cuts=[]\n",
    "\n",
    "for i in cuts:\n",
    "    df_cre=apply_cutoffs(df_cre,i[\"skew_cuts\"])\n",
    "    df_cre=apply_cutoffs(df_cre,i[\"ref_cuts\"])\n",
    "    \n",
    "    #all_cuts=all_cuts+i[\"ref_cuts\"]+i[\"skew_cuts\"]\n",
    "    all_cuts=all_cuts+[sublist[0] for sublist in i[\"skew_cuts\"]]\n",
    "    all_cuts=all_cuts+[sublist[0] for sublist in i[\"ref_cuts\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25577eb5-ad89-4016-9173-2799eb0525c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# perform actual count & sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f742f8-f008-4526-9fe5-99d4a4ea455b",
   "metadata": {},
   "source": [
    "Replace all commas and carats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "224c4514-43c9-41ed-8884-3314fe42f4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:27.211569Z",
     "iopub.status.busy": "2024-04-16T22:47:27.211352Z",
     "iopub.status.idle": "2024-04-16T22:47:31.684679Z",
     "shell.execute_reply": "2024-04-16T22:47:31.684193Z",
     "shell.execute_reply.started": "2024-04-16T22:47:27.211553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_types=[\"K562\",\"SKNSH\",\"HepG2\"]\n",
    "\n",
    "to_group_by=cadd_columns+cre_bool_columns+[\"category\",\"pleio\",\"phylop_significant\"]+[\"emVar_\"+i for i in cell_types]+all_cuts+[\"in_TF\"]\n",
    "renamed_column_map = {col: col.replace(',', '^').replace('.','&') for col in to_group_by}\n",
    "new_group=[col.replace(',', '^').replace('.','&') for col in to_group_by]\n",
    "\n",
    "for old_name, new_name in renamed_column_map.items():\n",
    "        df_cre = df_cre.withColumnRenamed(old_name, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9a42c-70ce-45b0-a6dc-42ab84c8b7f6",
   "metadata": {},
   "source": [
    "Make actual count table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df6075e-eaab-4b2d-abf3-618a5d3f3e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:31.685793Z",
     "iopub.status.busy": "2024-04-16T22:47:31.685567Z",
     "iopub.status.idle": "2024-04-16T22:47:31.872105Z",
     "shell.execute_reply": "2024-04-16T22:47:31.871637Z",
     "shell.execute_reply.started": "2024-04-16T22:47:31.685777Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_table = df_cre.groupBy(new_group).agg(\n",
    "    F.sum(\"P_ANNO\").alias(\"sum_phylop\"),\n",
    "    F.sum(F.col(\"P_ANNO\") * F.col(\"P_ANNO\")).alias(\"sum_of_squared_phylop\"),\n",
    "    F.count(\"*\").alias(\"count\")  # Count of elements in each group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26147d21-d18d-4038-8d93-9b3fe0cc567c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T22:47:31.872902Z",
     "iopub.status.busy": "2024-04-16T22:47:31.872746Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/16 18:47:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:===========================>                         (918 + 10) / 1776]\r"
     ]
    }
   ],
   "source": [
    "#note: this cell will take substantial time & resources to execute.\n",
    "\n",
    "count_table.cache()\n",
    "\n",
    "count_table.coalesce(1).write.csv(data_output_base+\"count_all.csv\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bf787-ac1b-4685-9cff-e5f0615015ce",
   "metadata": {},
   "source": [
    "# Subset & write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63909309-8741-4ab7-bc0f-ea733ecb1ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_sum_and_dump(name,spark_df,group_by_cols):\n",
    "    \n",
    "    temp=spark_df.groupBy(*group_by_cols).agg(\n",
    "        F.sum(\"sum_phylop\").alias(\"sum_sum_phylop\"),\n",
    "        F.sum(\"sum_of_squared_phylop\").alias(\"sum_sum_of_squared_phylop\"),\n",
    "        F.sum(\"count\").alias(\"sum_count\"),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp.coalesce(1).write.csv(data_output_base+name, mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3d1ff-d0f8-45b0-a9be-4f652ea02550",
   "metadata": {},
   "source": [
    "## phylop vs rarity vs genomic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d16db1-350e-4fad-ad41-7acb98cae183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_sum_and_dump(name=\"rarity_pleio\",\n",
    "                   spark_df=count_table,\n",
    "                  group_by_cols=[\"category\", \"pleio\"]+cre_bool_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff3e4a-b437-41ee-abc0-44c7cf320618",
   "metadata": {},
   "source": [
    "## phylop vs pleiotropy vs genomic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d275f6-d99d-4065-9003-6e0552c04961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T21:25:16.447141Z",
     "iopub.status.busy": "2024-04-16T21:25:16.446694Z",
     "iopub.status.idle": "2024-04-16T21:25:19.581619Z",
     "shell.execute_reply": "2024-04-16T21:25:19.581106Z",
     "shell.execute_reply.started": "2024-04-16T21:25:16.447124Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_sum_and_dump(name=\"phylop_pleio\",\n",
    "                   spark_df=count_table,\n",
    "                   group_by_cols=[\"pleio\", \"phylop_significant\"]+cre_bool_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeec5cc-b25b-4067-8cb9-9defba6acdf3",
   "metadata": {},
   "source": [
    "## phylop vs emvar status vs genomic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcdb1ff8-88c6-4ba5-b6a4-53c98523f764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T16:17:30.391500Z",
     "iopub.status.busy": "2024-02-26T16:17:30.391287Z",
     "iopub.status.idle": "2024-02-26T16:17:34.028563Z",
     "shell.execute_reply": "2024-02-26T16:17:34.028002Z",
     "shell.execute_reply.started": "2024-02-26T16:17:30.391486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_sum_and_dump(name=\"phylop_emvar\",\n",
    "                   spark_df=count_table,\n",
    "                   group_by_cols=[\"phylop_significant\"]+cre_bool_columns+emvar_bool_columns\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501b7d0-fa47-40b3-9366-d81467631a44",
   "metadata": {},
   "source": [
    "## cadd vs rarity vs genomic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14899b14-2371-4f94-82f0-4e92b5ca29b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T16:17:34.029552Z",
     "iopub.status.busy": "2024-02-26T16:17:34.029392Z",
     "iopub.status.idle": "2024-02-26T16:17:36.795058Z",
     "shell.execute_reply": "2024-02-26T16:17:36.794562Z",
     "shell.execute_reply.started": "2024-02-26T16:17:34.029537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_sum_and_dump(name=\"CADD_count_table\",\n",
    "                   spark_df=count_table,\n",
    "                   group_by_cols=[\"category\"]+cadd_columns+cre_bool_columns\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f8066-e964-4492-b62e-33820d8e862f",
   "metadata": {},
   "source": [
    "## cadd vs pleiotropy vs genomic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0f57e6d-0fef-422e-acd7-13961c074f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T16:17:36.795996Z",
     "iopub.status.busy": "2024-02-26T16:17:36.795697Z",
     "iopub.status.idle": "2024-02-26T16:17:39.162181Z",
     "shell.execute_reply": "2024-02-26T16:17:39.161700Z",
     "shell.execute_reply.started": "2024-02-26T16:17:36.795982Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "group_sum_and_dump(name=\"CADD_pleio\",\n",
    "                   spark_df=count_table,\n",
    "                   group_by_cols=[\"pleio\"]+cadd_columns+cre_bool_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a1954-2762-4099-b9b2-444adfa95ae2",
   "metadata": {},
   "source": [
    "## malinois skew vs malinois reference, (malinois both mean & per cell type) vs genomic regions vs rarity category\n",
    "\n",
    "We'll do different files for different cell-types (+ mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e90a0a3d-6207-43e4-acb3-03281d3c1ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T16:17:39.163724Z",
     "iopub.status.busy": "2024-02-26T16:17:39.163347Z",
     "iopub.status.idle": "2024-02-26T16:17:55.420124Z",
     "shell.execute_reply": "2024-02-26T16:17:55.419716Z",
     "shell.execute_reply.started": "2024-02-26T16:17:39.163708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#each item of `cuts` is a cell-type (plus mean)\n",
    "mean_cut=None\n",
    "mean_thresh=None\n",
    "\n",
    "for i in cuts:\n",
    "    celltype=i[\"skew_name\"].split(\"_\")[0]\n",
    "    \n",
    "    \n",
    "        \n",
    "    #rarity category & genomic regions\n",
    "    to_group_by=[\"category\"]+cre_bool_columns\n",
    "    #add skew & ref coulmns for current \n",
    "    to_group_by=to_group_by+get_column_names(i[\"skew_cuts\"])+get_column_names(i[\"ref_cuts\"])\n",
    "    \n",
    "    #remove illegal characters \n",
    "    to_group_by=[item.replace(',', '^').replace('.','&') for item in to_group_by]\n",
    "    \n",
    "    #save mean for later use\n",
    "    if celltype==\"mean\":\n",
    "        mean_cut=i\n",
    "        mean_thresh=to_group_by\n",
    "    \n",
    "    group_sum_and_dump(name=f\"malinois_{celltype}\",\n",
    "                       spark_df=count_table,\n",
    "                       group_by_cols=to_group_by)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
