{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Import relevant libraries",
      "metadata": {},
      "id": "0bb6d8d0-6787-4b3a-99b9-f7791fab66e5"
    },
    {
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nimport pyspark.sql.types as T\nimport os",
      "metadata": {
        "tags": [],
        "execution": {
          "shell.execute_reply": "2024-03-31T20:05:37.665473Z",
          "iopub.execute_input": "2024-03-31T20:05:37.663697Z",
          "iopub.status.busy": "2024-03-31T20:05:37.663226Z",
          "shell.execute_reply.started": "2024-03-31T20:05:37.663680Z",
          "iopub.status.idle": "2024-03-31T20:05:37.665815Z"
        }
      },
      "outputs": [],
      "id": "3fa77bf6-a81e-48e8-8ec2-01aa60bd81a5",
      "cell_type": "code",
      "execution_count": 12
    },
    {
      "id": "75c1fca1-5c01-47bb-b397-dd32048d9c40",
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create a spark session"
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/03/31 13:54:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "metadata": {
        "execution": {
          "shell.execute_reply": "2024-03-31T17:54:37.244551Z",
          "shell.execute_reply.started": "2024-03-31T17:54:31.307664Z",
          "iopub.execute_input": "2024-03-31T17:54:31.307681Z",
          "iopub.status.busy": "2024-03-31T17:54:31.307283Z",
          "iopub.status.idle": "2024-03-31T17:54:37.245024Z"
        },
        "tags": []
      },
      "id": "1aacb0c2-1975-4faf-a934-38a6bf93e21a",
      "source": "spark = SparkSession.builder.appName(\"add_TF\").getOrCreate()"
    },
    {
      "id": "cf6fc160-bedb-4f33-b6ec-9b32f5fb4b73",
      "cell_type": "markdown",
      "source": "Pick the chromosome to process",
      "metadata": {}
    },
    {
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "only crunching chromosome chr22\n"
          ],
          "name": "stdout"
        }
      ],
      "id": "e3a727a1-1dee-4a4a-b7ba-62b8d1bdf751",
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.idle": "2024-03-31T17:54:37.249003Z",
          "iopub.execute_input": "2024-03-31T17:54:37.246188Z",
          "shell.execute_reply.started": "2024-03-31T17:54:37.246172Z",
          "shell.execute_reply": "2024-03-31T17:54:37.248650Z",
          "iopub.status.busy": "2024-03-31T17:54:37.245962Z"
        }
      },
      "source": "chromosome=\"NONE\"\n\n\nif \"which_chr\" in os.environ:\n    chromosome = os.environ['which_chr']\n\nif chromosome==\"NONE\":\n    print(\"error : did not find which chromosome we are supposed to crunch!\")\n    exit(-1)\nelse:\n    print(\"only crunching chromosome \"+chromosome)",
      "cell_type": "code"
    },
    {
      "source": "Load the chromosome",
      "metadata": {},
      "id": "ad08c2f7-80a6-41ae-ae8a-71bb35665544",
      "cell_type": "markdown"
    },
    {
      "execution_count": 4,
      "id": "2b7360fa-7209-426c-bc50-dbc88b1d4c8f",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/03/31 13:54:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
            "                                                                                \r"
          ]
        }
      ],
      "cell_type": "code",
      "source": "variant_path=f\"/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_data/3.0pleio_and_filter/{chromosome}/*.csv.gz\"\n\nvariants=spark.read.option(\"delimiter\",\"\\t\").csv(variant_path, header=True, inferSchema=True)",
      "metadata": {
        "tags": [],
        "execution": {
          "shell.execute_reply.started": "2024-03-31T17:54:37.249701Z",
          "iopub.status.busy": "2024-03-31T17:54:37.249492Z",
          "iopub.execute_input": "2024-03-31T17:54:37.249715Z",
          "shell.execute_reply": "2024-03-31T17:54:55.357784Z",
          "iopub.status.idle": "2024-03-31T17:54:55.358310Z"
        }
      }
    },
    {
      "id": "7e303ebd-085b-47dd-ab27-0402a022e4ab",
      "execution_count": 27,
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.execute_input": "2024-03-31T20:34:51.705232Z",
          "shell.execute_reply.started": "2024-03-31T20:34:51.705215Z",
          "iopub.status.idle": "2024-03-31T20:34:51.709285Z",
          "iopub.status.busy": "2024-03-31T20:34:51.704785Z",
          "shell.execute_reply": "2024-03-31T20:34:51.708953Z"
        }
      },
      "cell_type": "code",
      "source": "variants",
      "outputs": [
        {
          "execution_count": 27,
          "data": {
            "text/plain": [
              "DataFrame[CHROM: string, POS: int, ID: string, REF: string, ALT: string, QUAL: string, FILTER: string, INFO: string, K562__ref: double, HepG2__ref: double, SKNSH__ref: double, K562__alt: double, HepG2__alt: double, SKNSH__alt: double, K562__skew: double, HepG2__skew: double, SKNSH__skew: double, AC: int, AN: int, AF: double, cadd_phred: double, is_in_dELS: boolean, is_in_CA: boolean, is_in_pELS: boolean, is_in_CA-H3K4me3: boolean, is_in_CA-CTCF: boolean, is_in_PLS: boolean, is_in_TF: boolean, is_in_CA-TF: boolean, P_ANNO: double, mean_ref: double, mean_skew: double, MAF: double, category: string, emVar_K562: boolean, emVar_SKNSH: boolean, emVar_HepG2: boolean, pleio: int]"
            ]
          },
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "id": "8b4b0090-08ce-4bf3-be23-77bf1c28a5c9",
      "source": "Load the TF file.\n\nFile from Dr. Steven Rong, \nTEMP: see slack #var-eff-reilly, 2024-03-13"
    },
    {
      "id": "4f02b8e4-5589-4855-8ed1-bb0ff89dfbdf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T20:16:33.650808Z",
          "iopub.status.busy": "2024-03-31T20:16:33.650361Z",
          "shell.execute_reply": "2024-03-31T20:16:33.789069Z",
          "iopub.status.idle": "2024-03-31T20:16:33.789482Z",
          "shell.execute_reply.started": "2024-03-31T20:16:33.650792Z"
        },
        "tags": []
      },
      "source": "tf_footprint_path=\"/gpfs/gibbs/pi/reilly/VariantEffects/scripts/noon_data/consensus_footprints_and_motifs_hg38_GRanges.txt.gz\"\n\ntf_footprints=spark.read.option(\"delimiter\", \"\\t\").csv(tf_footprint_path, header=True, inferSchema=False)",
      "execution_count": 14,
      "cell_type": "code",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "id": "1b439c29-090c-4f7c-b824-3534628adfb4",
      "source": "Our variant file (the chromosome we just loaded) was originally a VCF, which is 1-based.\n\nIs our TF footprint file 0 or 1 based? I would *guess* 0-based, since it is bed-y.\n\nRecall:\n\n```\nChrZ        T   A   C   G   T\n          | | | | | | | | | |\n1 based   | 1 | 2 | 3 | 4 | 5\n0 based   0   1   2   3   4\n```\n\n1 based vs 1-based\n\nBy inspection, we see,\n\n1-based : length=end-start+1\n0-based : length=end-start\n\n\nOur first two rows have:\n\n\n```\n\nseqnames        start   end     width   strand  identifier      mean_signal     num_samples     num_fps summitcore_start       core_end        motif_clusters\nchr1    180788  180831  44      *       1.100643.4      92.527889       6       6       180802  180784  180808RREB1_MA0073.1;RREB1_MA0073.1;RREB1_MA0073.1;RARA_MOUSE.H11MO.0.A;TBX1_TBX_1;RR\\\nEB1_MA0073.1;ZNF524_C2H2_1;TBX20_TBX_1;TBX20_TBX_5;RARA+RXRG_MA1149.1;RREB1_MA0073.1;RREB1_MA0073.1;RREB1_MA0073.1;RREB1_MA0073.1;RREB1_MA0073.1\n\n```\n\nThe formtting here is a little weird. 'seqnames' seems to refer to chromosome.\n\nend-start=180831-180788=43\n\nend-start+1 = 43+1 = width\n\nSo it's 1-based.\n\nintersection : vcf_coord>=start && vcf_coord <= end\n\neasy!\n"
    },
    {
      "metadata": {
        "execution": {
          "shell.execute_reply": "2024-03-31T19:43:15.336183Z",
          "iopub.status.idle": "2024-03-31T19:43:15.336521Z",
          "shell.execute_reply.started": "2024-03-31T19:43:15.333481Z",
          "iopub.status.busy": "2024-03-31T19:43:15.333031Z",
          "iopub.execute_input": "2024-03-31T19:43:15.333498Z"
        },
        "tags": []
      },
      "cell_type": "markdown",
      "id": "af82981b-5c08-437e-9544-cc10b12ce303",
      "source": "Cast."
    },
    {
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T20:16:47.166362Z",
          "iopub.status.idle": "2024-03-31T20:16:47.190360Z",
          "shell.execute_reply.started": "2024-03-31T20:16:47.166769Z",
          "shell.execute_reply": "2024-03-31T20:16:47.189981Z",
          "iopub.execute_input": "2024-03-31T20:16:47.166786Z"
        },
        "tags": []
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 16,
      "id": "431601cd-406e-48f7-a8d2-9e8ac80593a5",
      "source": "int_columns = [\"start\", \"end\"]\n\n# Cast integer columns\nfor column in int_columns:\n    tf_footprints = tf_footprints.withColumn(column, F.col(column).cast(T.IntegerType()))"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "metadata": {},
          "execution_count": 17,
          "data": {
            "text/plain": [
              "DataFrame[seqnames: string, start: int, end: int, width: string, strand: string, identifier: string, mean_signal: string, num_samples: string, num_fps: string, summit: string, core_start: string, core_end: string, motif_clusters: string]"
            ]
          },
          "output_type": "execute_result"
        }
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.idle": "2024-03-31T20:16:52.732267Z",
          "iopub.execute_input": "2024-03-31T20:16:52.728654Z",
          "shell.execute_reply": "2024-03-31T20:16:52.731949Z",
          "iopub.status.busy": "2024-03-31T20:16:52.728207Z",
          "shell.execute_reply.started": "2024-03-31T20:16:52.728638Z"
        }
      },
      "execution_count": 17,
      "id": "4a9b92a8-192f-4e8f-bdf1-05e8c84d1c73",
      "source": "tf_footprints"
    },
    {
      "cell_type": "markdown",
      "id": "8d414b32-0385-4213-ad0e-0e968f61dd1a",
      "source": "Subset the dataframe to the relevant portion. ",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "execution": {
          "shell.execute_reply": "2024-03-31T20:25:27.406969Z",
          "iopub.execute_input": "2024-03-31T20:25:27.390952Z",
          "iopub.status.busy": "2024-03-31T20:25:27.390730Z",
          "shell.execute_reply.started": "2024-03-31T20:25:27.390937Z",
          "iopub.status.idle": "2024-03-31T20:25:27.407349Z"
        },
        "tags": []
      },
      "id": "a90acf1c-84a3-4199-8ef1-aea79be4c10c",
      "execution_count": 19,
      "source": "# List of columns I want to keep\ncolumns_to_keep = [\"seqnames\", \"start\", \"end\"]\n\ntf_footprints = tf_footprints.select(*columns_to_keep).filter(tf_footprints[\"seqnames\"] == chromosome)\n"
    },
    {
      "id": "239635de-1675-4cfe-a317-5fd9bdb94121",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-03-31T20:31:12.768633Z",
          "iopub.status.idle": "2024-03-31T20:31:12.775230Z",
          "shell.execute_reply": "2024-03-31T20:31:12.774883Z",
          "shell.execute_reply.started": "2024-03-31T20:31:12.769013Z",
          "iopub.execute_input": "2024-03-31T20:31:12.769028Z"
        }
      },
      "source": "tf_footprints_broadcast = F.broadcast(tf_footprints)",
      "cell_type": "code",
      "outputs": []
    },
    {
      "source": "#employing a simple strategy where we make a table with every TF range\n#in a row with every (cartesian product) then just delete those that don't fall in \n#the range. This is algorithmically boneheaded (much better algo exist w/ better time complexity), \n#but since one of the tables is small we can broadcast is which is fast. \n#so it works fine. \njoined_df = variants.crossJoin(tf_footprints_broadcast) \\\n    .filter((F.col(\"POS\") >= F.col(\"start\")) & (F.col(\"POS\") <= F.col(\"end\")))",
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "execution": {
          "shell.execute_reply.started": "2024-03-31T20:35:18.464204Z",
          "shell.execute_reply": "2024-03-31T20:35:18.481762Z",
          "iopub.status.idle": "2024-03-31T20:35:18.482147Z",
          "iopub.execute_input": "2024-03-31T20:35:18.464220Z",
          "iopub.status.busy": "2024-03-31T20:35:18.463809Z"
        },
        "tags": []
      },
      "id": "49c24336-b2fc-4837-8285-165adb22d558",
      "execution_count": 29
    },
    {
      "id": "5cc4b274-b553-4d60-9d79-2b8410211a2c",
      "source": "#now we simply select those positions that did fall in a TF\npos_in_TF = joined_df.select(\"POS\").distinct()",
      "execution_count": 36,
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "execution": {
          "shell.execute_reply.started": "2024-03-31T20:56:04.414811Z",
          "shell.execute_reply": "2024-03-31T20:56:04.423964Z",
          "iopub.execute_input": "2024-03-31T20:56:04.414827Z",
          "iopub.status.busy": "2024-03-31T20:56:04.414557Z",
          "iopub.status.idle": "2024-03-31T20:56:04.424360Z"
        }
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#and annotate the original variants dataframe with that information\n\n#add a simple TRUE for variants in at TF motif\npos_in_TF = pos_in_TF.withColumn(\"in_TF\", F.lit(True))\n\nvariants_annotated = variants.join(pos_in_TF, on=\"POS\", how=\"left\")\n\n#those positions not found to be in any interval shold be false!\nvariants_annotated = variants_annotated.fillna({'in_TF': False})",
      "outputs": [],
      "metadata": {
        "tags": [],
        "execution": {
          "shell.execute_reply": "2024-03-31T20:56:05.024450Z",
          "iopub.status.idle": "2024-03-31T20:56:05.024865Z",
          "iopub.status.busy": "2024-03-31T20:56:04.986242Z",
          "iopub.execute_input": "2024-03-31T20:56:04.986489Z",
          "shell.execute_reply.started": "2024-03-31T20:56:04.986474Z"
        }
      },
      "id": "c7c99579-fe90-4527-b001-b1bbdfba6c3b",
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": "#dump the data back to disc. \noutput_root=\"/home/mcn26/varef/scripts/noon_data/3.5add_TF_footprints/\"\n\nvariants_annotated.write \\\n    .option(\"header\",\"true\") \\\n    .option(\"delimiter\",\"\\t\") \\\n    .option(\"compression\", \"gzip\") \\\n    .csv(output_root+chromosome)\n\nspark.stop()",
      "id": "e12c69a8-59b0-4ed9-954b-6a3d63478f86",
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "version": "3.10.12",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}